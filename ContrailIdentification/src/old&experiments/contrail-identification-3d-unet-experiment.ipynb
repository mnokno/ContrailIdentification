{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13894.230244,
   "end_time": "2023-05-31T14:05:20.971463",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-31T10:13:46.741219",
   "version": "2.4.0"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.012379,
     "end_time": "2023-05-31T10:13:57.071000",
     "exception": false,
     "start_time": "2023-05-31T10:13:57.058621",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from matplotlib import animation\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "papermill": {
     "duration": 35.955329,
     "end_time": "2023-05-31T10:14:33.038535",
     "exception": false,
     "start_time": "2023-05-31T10:13:57.083206",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-06-04T22:28:55.643242Z",
     "iopub.execute_input": "2023-06-04T22:28:55.643604Z",
     "iopub.status.idle": "2023-06-04T22:28:59.341558Z",
     "shell.execute_reply.started": "2023-06-04T22:28:55.643575Z",
     "shell.execute_reply": "2023-06-04T22:28:59.340673Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install torchsummary\n",
    "from torchsummary import summary"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-04T22:28:59.343748Z",
     "iopub.execute_input": "2023-06-04T22:28:59.344387Z",
     "iopub.status.idle": "2023-06-04T22:29:11.311703Z",
     "shell.execute_reply.started": "2023-06-04T22:28:59.344353Z",
     "shell.execute_reply": "2023-06-04T22:29:11.310491Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing Utilites"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.011621,
     "end_time": "2023-05-31T10:14:33.062449",
     "exception": false,
     "start_time": "2023-05-31T10:14:33.050828",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data_dir: str = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.022017,
     "end_time": "2023-05-31T10:14:33.096172",
     "exception": false,
     "start_time": "2023-05-31T10:14:33.074155",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-06-04T22:29:11.314769Z",
     "iopub.execute_input": "2023-06-04T22:29:11.316758Z",
     "iopub.status.idle": "2023-06-04T22:29:11.322385Z",
     "shell.execute_reply.started": "2023-06-04T22:29:11.316711Z",
     "shell.execute_reply": "2023-06-04T22:29:11.321123Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_train_idx = pd.DataFrame({'idx': os.listdir('/kaggle/input/google-research-identify-contrails-reduce-global-warming/train')})\n",
    "df_validation_idx = pd.DataFrame({'idx': os.listdir('/kaggle/input/google-research-identify-contrails-reduce-global-warming/validation')})\n",
    "df_test_idx = pd.DataFrame({'idx': os.listdir('/kaggle/input/google-research-identify-contrails-reduce-global-warming/test')})"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.371282,
     "end_time": "2023-05-31T10:14:33.479090",
     "exception": false,
     "start_time": "2023-05-31T10:14:33.107808",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-06-04T22:29:11.326324Z",
     "iopub.execute_input": "2023-06-04T22:29:11.327265Z",
     "iopub.status.idle": "2023-06-04T22:29:11.783498Z",
     "shell.execute_reply.started": "2023-06-04T22:29:11.327227Z",
     "shell.execute_reply": "2023-06-04T22:29:11.782581Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_band_images(idx: str, parrent_folder: str, band: str) -> np.array:\n",
    "    return np.load(os.path.join(data_dir, parrent_folder, idx, f'band_{band}.npy'))"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.020039,
     "end_time": "2023-05-31T10:14:33.511727",
     "exception": false,
     "start_time": "2023-05-31T10:14:33.491688",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-06-04T22:29:11.786260Z",
     "iopub.execute_input": "2023-06-04T22:29:11.786907Z",
     "iopub.status.idle": "2023-06-04T22:29:11.792596Z",
     "shell.execute_reply.started": "2023-06-04T22:29:11.786863Z",
     "shell.execute_reply": "2023-06-04T22:29:11.791755Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "_T11_BOUNDS = (243, 303)\n",
    "_CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n",
    "_TDIFF_BOUNDS = (-4, 2)\n",
    "\n",
    "def normalize_range(data, bounds):\n",
    "    \"\"\"Maps data to the range [0, 1].\"\"\"\n",
    "    return (data - bounds[0]) / (bounds[1] - bounds[0])\n",
    "\n",
    "\n",
    "def get_ash_color_images(idx: str, parrent_folder: str, get_mask_frame_only=False) -> np.array:\n",
    "    band11 = get_band_images(idx, parrent_folder, '11')\n",
    "    band14 = get_band_images(idx, parrent_folder, '14')\n",
    "    band15 = get_band_images(idx, parrent_folder, '15')\n",
    "    \n",
    "    if get_mask_frame_only:\n",
    "        band11 = band11[:,:,4]\n",
    "        band14 = band14[:,:,4]\n",
    "        band15 = band15[:,:,4]\n",
    "\n",
    "    r = normalize_range(band15 - band14, _TDIFF_BOUNDS)\n",
    "    g = normalize_range(band14 - band11, _CLOUD_TOP_TDIFF_BOUNDS)\n",
    "    b = normalize_range(band14, _T11_BOUNDS)\n",
    "    false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n",
    "    return false_color"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.023106,
     "end_time": "2023-05-31T10:14:33.546552",
     "exception": false,
     "start_time": "2023-05-31T10:14:33.523446",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-06-04T22:29:11.794203Z",
     "iopub.execute_input": "2023-06-04T22:29:11.794970Z",
     "iopub.status.idle": "2023-06-04T22:29:11.805262Z",
     "shell.execute_reply.started": "2023-06-04T22:29:11.794939Z",
     "shell.execute_reply": "2023-06-04T22:29:11.804351Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_mask_image(idx: str, parrent_folder: str) -> np.array:\n",
    "    return np.load(os.path.join(data_dir, parrent_folder, idx, 'human_pixel_masks.npy')) "
   ],
   "metadata": {
    "papermill": {
     "duration": 0.018892,
     "end_time": "2023-05-31T10:14:33.577019",
     "exception": false,
     "start_time": "2023-05-31T10:14:33.558127",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-06-04T22:29:11.806941Z",
     "iopub.execute_input": "2023-06-04T22:29:11.807672Z",
     "iopub.status.idle": "2023-06-04T22:29:11.814738Z",
     "shell.execute_reply.started": "2023-06-04T22:29:11.807640Z",
     "shell.execute_reply": "2023-06-04T22:29:11.813885Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.190797,
     "end_time": "2023-05-31T10:17:34.693167",
     "exception": false,
     "start_time": "2023-05-31T10:17:34.502370",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.259205,
     "end_time": "2023-05-31T10:17:35.141967",
     "exception": false,
     "start_time": "2023-05-31T10:17:34.882762",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-06-04T22:29:11.816196Z",
     "iopub.execute_input": "2023-06-04T22:29:11.816922Z",
     "iopub.status.idle": "2023-06-04T22:29:11.862552Z",
     "shell.execute_reply.started": "2023-06-04T22:29:11.816892Z",
     "shell.execute_reply": "2023-06-04T22:29:11.861612Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "execution_count": 8,
     "output_type": "execute_result",
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super(Up, self).__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        # Define your layers\n",
    "        self.inc = DoubleConv(24, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 256)\n",
    "        self.up2 = Up(512, 128)\n",
    "        self.up3 = Up(256, 64)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the layers\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return x"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.217927,
     "end_time": "2023-05-31T10:17:35.546217",
     "exception": false,
     "start_time": "2023-05-31T10:17:35.328290",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-06-04T22:29:11.864692Z",
     "iopub.execute_input": "2023-06-04T22:29:11.865437Z",
     "iopub.status.idle": "2023-06-04T22:29:11.883315Z",
     "shell.execute_reply.started": "2023-06-04T22:29:11.865405Z",
     "shell.execute_reply": "2023-06-04T22:29:11.882556Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class DoubleConv3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding):\n",
    "        super(DoubleConv3D, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "class Down3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down3D, self).__init__()\n",
    "        self.maxpool_conv3d = nn.Sequential(\n",
    "            nn.AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "            DoubleConv3D(in_channels, out_channels, 3, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv3d(x)\n",
    "    \n",
    "class TestNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestNet, self).__init__()\n",
    "        # Define your layers\n",
    "        self.conv3d = DoubleConv3D(3, 16, 3, 1)\n",
    "        self.down3d_1 = Down3D(16, 32)\n",
    "        self.down3d_2 = Down3D(32, 64)\n",
    "        self.down3d_3 = Down3D(64, 128)\n",
    "        \n",
    "        self.conv2d = DoubleConv(24, 64)\n",
    "        self.down1 = Down(16*8 + 64, 128)\n",
    "        self.down2 = Down(32*8 + 128, 256)\n",
    "        self.down3 = Down(64*8 + 256, 512)\n",
    "        self.down4 = Down(128*8 + 512, 512)\n",
    "        \n",
    "        self.up1 = Up(1024, 256)\n",
    "        self.up2 = Up(512, 128)\n",
    "        self.up3 = Up(256, 64)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x0_2d = self.conv2d(x.view(-1, x.size(1) * x.size(2), x.size(3), x.size(4)))\n",
    "        \n",
    "        x_3d = self.conv3d(x)\n",
    "        x1_2d = self.down1(torch.cat((x_3d.view(-1, x_3d.size(1) * x_3d.size(2), x_3d.size(3), x_3d.size(4)), x0_2d), dim=1))\n",
    "        \n",
    "        x_3d = self.down3d_1(x_3d)\n",
    "        x2_2d = self.down2(torch.cat((x_3d.view(-1, x_3d.size(1) * x_3d.size(2), x_3d.size(3), x_3d.size(4)), x1_2d), dim=1))\n",
    "        \n",
    "        x_3d = self.down3d_2(x_3d)\n",
    "        x3_2d = self.down3(torch.cat((x_3d.view(-1, x_3d.size(1) * x_3d.size(2), x_3d.size(3), x_3d.size(4)), x2_2d), dim=1))\n",
    "        \n",
    "        x_3d = self.down3d_3(x_3d)\n",
    "        x4_2d = self.down4(torch.cat((x_3d.view(-1, x_3d.size(1) * x_3d.size(2), x_3d.size(3), x_3d.size(4)), x3_2d), dim=1))\n",
    "        \n",
    "        x = self.up1(x4_2d, x3_2d)\n",
    "        x = self.up2(x, x2_2d)\n",
    "        x = self.up3(x, x1_2d)\n",
    "        x = self.up4(x, x0_2d)\n",
    "        x = self.outc(x)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-04T22:29:11.888690Z",
     "iopub.execute_input": "2023-06-04T22:29:11.889228Z",
     "iopub.status.idle": "2023-06-04T22:29:11.907798Z",
     "shell.execute_reply.started": "2023-06-04T22:29:11.889192Z",
     "shell.execute_reply": "2023-06-04T22:29:11.906939Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "summary(TestNet().to(device), input_size=(3, 8, 256, 256))"
   ],
   "metadata": {
    "papermill": {
     "duration": 7.975255,
     "end_time": "2023-05-31T10:17:43.707638",
     "exception": false,
     "start_time": "2023-05-31T10:17:35.732383",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-06-04T22:29:11.909084Z",
     "iopub.execute_input": "2023-06-04T22:29:11.909513Z",
     "iopub.status.idle": "2023-06-04T22:29:20.229199Z",
     "shell.execute_reply.started": "2023-06-04T22:29:11.909482Z",
     "shell.execute_reply": "2023-06-04T22:29:20.228123Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 256, 256]          13,888\n       BatchNorm2d-2         [-1, 64, 256, 256]             128\n              ReLU-3         [-1, 64, 256, 256]               0\n            Conv2d-4         [-1, 64, 256, 256]          36,928\n       BatchNorm2d-5         [-1, 64, 256, 256]             128\n              ReLU-6         [-1, 64, 256, 256]               0\n        DoubleConv-7         [-1, 64, 256, 256]               0\n            Conv3d-8      [-1, 16, 8, 256, 256]           1,312\n       BatchNorm3d-9      [-1, 16, 8, 256, 256]              32\n             ReLU-10      [-1, 16, 8, 256, 256]               0\n           Conv3d-11      [-1, 16, 8, 256, 256]           6,928\n      BatchNorm3d-12      [-1, 16, 8, 256, 256]              32\n             ReLU-13      [-1, 16, 8, 256, 256]               0\n     DoubleConv3D-14      [-1, 16, 8, 256, 256]               0\n        MaxPool2d-15        [-1, 192, 128, 128]               0\n           Conv2d-16        [-1, 128, 128, 128]         221,312\n      BatchNorm2d-17        [-1, 128, 128, 128]             256\n             ReLU-18        [-1, 128, 128, 128]               0\n           Conv2d-19        [-1, 128, 128, 128]         147,584\n      BatchNorm2d-20        [-1, 128, 128, 128]             256\n             ReLU-21        [-1, 128, 128, 128]               0\n       DoubleConv-22        [-1, 128, 128, 128]               0\n             Down-23        [-1, 128, 128, 128]               0\n        AvgPool3d-24      [-1, 16, 8, 128, 128]               0\n           Conv3d-25      [-1, 32, 8, 128, 128]          13,856\n      BatchNorm3d-26      [-1, 32, 8, 128, 128]              64\n             ReLU-27      [-1, 32, 8, 128, 128]               0\n           Conv3d-28      [-1, 32, 8, 128, 128]          27,680\n      BatchNorm3d-29      [-1, 32, 8, 128, 128]              64\n             ReLU-30      [-1, 32, 8, 128, 128]               0\n     DoubleConv3D-31      [-1, 32, 8, 128, 128]               0\n           Down3D-32      [-1, 32, 8, 128, 128]               0\n        MaxPool2d-33          [-1, 384, 64, 64]               0\n           Conv2d-34          [-1, 256, 64, 64]         884,992\n      BatchNorm2d-35          [-1, 256, 64, 64]             512\n             ReLU-36          [-1, 256, 64, 64]               0\n           Conv2d-37          [-1, 256, 64, 64]         590,080\n      BatchNorm2d-38          [-1, 256, 64, 64]             512\n             ReLU-39          [-1, 256, 64, 64]               0\n       DoubleConv-40          [-1, 256, 64, 64]               0\n             Down-41          [-1, 256, 64, 64]               0\n        AvgPool3d-42        [-1, 32, 8, 64, 64]               0\n           Conv3d-43        [-1, 64, 8, 64, 64]          55,360\n      BatchNorm3d-44        [-1, 64, 8, 64, 64]             128\n             ReLU-45        [-1, 64, 8, 64, 64]               0\n           Conv3d-46        [-1, 64, 8, 64, 64]         110,656\n      BatchNorm3d-47        [-1, 64, 8, 64, 64]             128\n             ReLU-48        [-1, 64, 8, 64, 64]               0\n     DoubleConv3D-49        [-1, 64, 8, 64, 64]               0\n           Down3D-50        [-1, 64, 8, 64, 64]               0\n        MaxPool2d-51          [-1, 768, 32, 32]               0\n           Conv2d-52          [-1, 512, 32, 32]       3,539,456\n      BatchNorm2d-53          [-1, 512, 32, 32]           1,024\n             ReLU-54          [-1, 512, 32, 32]               0\n           Conv2d-55          [-1, 512, 32, 32]       2,359,808\n      BatchNorm2d-56          [-1, 512, 32, 32]           1,024\n             ReLU-57          [-1, 512, 32, 32]               0\n       DoubleConv-58          [-1, 512, 32, 32]               0\n             Down-59          [-1, 512, 32, 32]               0\n        AvgPool3d-60        [-1, 64, 8, 32, 32]               0\n           Conv3d-61       [-1, 128, 8, 32, 32]         221,312\n      BatchNorm3d-62       [-1, 128, 8, 32, 32]             256\n             ReLU-63       [-1, 128, 8, 32, 32]               0\n           Conv3d-64       [-1, 128, 8, 32, 32]         442,496\n      BatchNorm3d-65       [-1, 128, 8, 32, 32]             256\n             ReLU-66       [-1, 128, 8, 32, 32]               0\n     DoubleConv3D-67       [-1, 128, 8, 32, 32]               0\n           Down3D-68       [-1, 128, 8, 32, 32]               0\n        MaxPool2d-69         [-1, 1536, 16, 16]               0\n           Conv2d-70          [-1, 512, 16, 16]       7,078,400\n      BatchNorm2d-71          [-1, 512, 16, 16]           1,024\n             ReLU-72          [-1, 512, 16, 16]               0\n           Conv2d-73          [-1, 512, 16, 16]       2,359,808\n      BatchNorm2d-74          [-1, 512, 16, 16]           1,024\n             ReLU-75          [-1, 512, 16, 16]               0\n       DoubleConv-76          [-1, 512, 16, 16]               0\n             Down-77          [-1, 512, 16, 16]               0\n         Upsample-78          [-1, 512, 32, 32]               0\n           Conv2d-79          [-1, 256, 32, 32]       2,359,552\n      BatchNorm2d-80          [-1, 256, 32, 32]             512\n             ReLU-81          [-1, 256, 32, 32]               0\n           Conv2d-82          [-1, 256, 32, 32]         590,080\n      BatchNorm2d-83          [-1, 256, 32, 32]             512\n             ReLU-84          [-1, 256, 32, 32]               0\n       DoubleConv-85          [-1, 256, 32, 32]               0\n               Up-86          [-1, 256, 32, 32]               0\n         Upsample-87          [-1, 256, 64, 64]               0\n           Conv2d-88          [-1, 128, 64, 64]         589,952\n      BatchNorm2d-89          [-1, 128, 64, 64]             256\n             ReLU-90          [-1, 128, 64, 64]               0\n           Conv2d-91          [-1, 128, 64, 64]         147,584\n      BatchNorm2d-92          [-1, 128, 64, 64]             256\n             ReLU-93          [-1, 128, 64, 64]               0\n       DoubleConv-94          [-1, 128, 64, 64]               0\n               Up-95          [-1, 128, 64, 64]               0\n         Upsample-96        [-1, 128, 128, 128]               0\n           Conv2d-97         [-1, 64, 128, 128]         147,520\n      BatchNorm2d-98         [-1, 64, 128, 128]             128\n             ReLU-99         [-1, 64, 128, 128]               0\n          Conv2d-100         [-1, 64, 128, 128]          36,928\n     BatchNorm2d-101         [-1, 64, 128, 128]             128\n            ReLU-102         [-1, 64, 128, 128]               0\n      DoubleConv-103         [-1, 64, 128, 128]               0\n              Up-104         [-1, 64, 128, 128]               0\n        Upsample-105         [-1, 64, 256, 256]               0\n          Conv2d-106         [-1, 64, 256, 256]          73,792\n     BatchNorm2d-107         [-1, 64, 256, 256]             128\n            ReLU-108         [-1, 64, 256, 256]               0\n          Conv2d-109         [-1, 64, 256, 256]          36,928\n     BatchNorm2d-110         [-1, 64, 256, 256]             128\n            ReLU-111         [-1, 64, 256, 256]               0\n      DoubleConv-112         [-1, 64, 256, 256]               0\n              Up-113         [-1, 64, 256, 256]               0\n          Conv2d-114          [-1, 1, 256, 256]              65\n================================================================\nTotal params: 22,103,153\nTrainable params: 22,103,153\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 6.00\nForward/backward pass size (MB): 1853.50\nParams size (MB): 84.32\nEstimated Total Size (MB): 1943.82\n----------------------------------------------------------------\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trainer"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.18571,
     "end_time": "2023-05-31T10:17:44.082550",
     "exception": false,
     "start_time": "2023-05-31T10:17:43.896840",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Dice(nn.Module):\n",
    "    def __init__(self, use_sigmoid=True):\n",
    "        super(Dice, self).__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.use_sigmoid = use_sigmoid\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        if self.use_sigmoid:\n",
    "            inputs = self.sigmoid(inputs)       \n",
    "        \n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2.0 *intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return dice\n",
    "    \n",
    "dice = Dice()"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.230206,
     "end_time": "2023-05-31T10:17:44.501249",
     "exception": false,
     "start_time": "2023-05-31T10:17:44.271043",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-06-04T22:29:20.231011Z",
     "iopub.execute_input": "2023-06-04T22:29:20.231701Z",
     "iopub.status.idle": "2023-06-04T22:29:20.239650Z",
     "shell.execute_reply.started": "2023-06-04T22:29:20.231666Z",
     "shell.execute_reply": "2023-06-04T22:29:20.238705Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class MyTrainer:\n",
    "    def __init__(self, model, optimizer, loss_fn, lr_scheduler):\n",
    "        self.validation_losses = []\n",
    "        self.batch_losses = []\n",
    "        self.epoch_losses = []\n",
    "        self.learning_rates = []\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self._check_optim_net_aligned()\n",
    "\n",
    "    # Ensures that the given optimizer points to the given model\n",
    "    def _check_optim_net_aligned(self):\n",
    "        assert self.optimizer.param_groups[0]['params'] == list(self.model.parameters())\n",
    "\n",
    "    # Trains the model\n",
    "    def fit(self,\n",
    "            train_dataloader: DataLoader,\n",
    "            test_dataloader: DataLoader,\n",
    "            epochs: int = 10,\n",
    "            eval_every: int = 1,\n",
    "            ):\n",
    "  \n",
    "        for e in range(epochs):\n",
    "            print(\"New learning rate: {}\".format(self.lr_scheduler.get_last_lr()))\n",
    "            self.learning_rates.append(self.lr_scheduler.get_last_lr()[0])\n",
    "\n",
    "            # Stores data about the batch\n",
    "            batch_losses = []\n",
    "            sub_batch_losses = []\n",
    "\n",
    "            for i, data in enumerate(train_dataloader):\n",
    "                self.model.train()\n",
    "                if i % 100 == 0:\n",
    "                    print(f'epotch: {e} batch: {i}/{len(train_dataloader)} loss: {torch.Tensor(sub_batch_losses).mean()}')\n",
    "                    sub_batch_losses.clear()\n",
    "                # Every data instance is an input + label pair\n",
    "                images, mask = data\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    images = images.cuda()\n",
    "                    mask = mask.cuda()\n",
    "\n",
    "                # Zero your gradients for every batch!\n",
    "                self.optimizer.zero_grad()\n",
    "                # Make predictions for this batch\n",
    "                outputs = self.model(images)\n",
    "                # Compute the loss and its gradients\n",
    "                loss = self.loss_fn(outputs, mask)\n",
    "                loss.backward()\n",
    "                # Adjust learning weights\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Saves data\n",
    "                self.batch_losses.append(loss.item())\n",
    "                batch_losses.append(loss)\n",
    "                sub_batch_losses.append(loss)\n",
    "\n",
    "            # Adjusts learning rate\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "\n",
    "            # Reports on the path\n",
    "            mean_epoch_loss = torch.Tensor(batch_losses).mean()\n",
    "            self.epoch_losses.append(mean_epoch_loss.item())\n",
    "            print('Train Epoch: {} Average Loss: {:.6f}'.format(e, mean_epoch_loss))\n",
    "\n",
    "            # Reports on the training progress\n",
    "            if (e + 1) % eval_every == 0:\n",
    "                torch.save(self.model.state_dict(), \"model_checkpoint_e\" + str(e) + \".pt\")\n",
    "                with torch.no_grad():\n",
    "                    self.model.eval()\n",
    "                    losses = []\n",
    "                    for i, data in enumerate(test_dataloader):\n",
    "                        # Every data instance is an input + label pair\n",
    "                        images, mask = data\n",
    "\n",
    "                        if torch.cuda.is_available():\n",
    "                            images = images.cuda()\n",
    "                            mask = mask.cuda()\n",
    "\n",
    "                        output = self.model(images)\n",
    "                        loss = self.loss_fn(output, mask)\n",
    "                        losses.append(loss.item())\n",
    "                        \n",
    "                    avg_loss = torch.Tensor(losses).mean().item()\n",
    "                    self.validation_losses.append(avg_loss)\n",
    "                    print(\"Validation loss after\", (e + 1), \"epochs was\", round(avg_loss, 4))"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.209594,
     "end_time": "2023-05-31T10:17:44.896218",
     "exception": false,
     "start_time": "2023-05-31T10:17:44.686624",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-06-04T22:29:20.242845Z",
     "iopub.execute_input": "2023-06-04T22:29:20.243143Z",
     "iopub.status.idle": "2023-06-04T22:29:20.261660Z",
     "shell.execute_reply.started": "2023-06-04T22:29:20.243111Z",
     "shell.execute_reply": "2023-06-04T22:29:20.260523Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.188458,
     "end_time": "2023-05-31T10:17:45.274679",
     "exception": false,
     "start_time": "2023-05-31T10:17:45.086221",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ContrailsAshDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, parrent_folder: str):\n",
    "        self.df_idx: pd.DataFrame = pd.DataFrame({'idx': os.listdir(f'/kaggle/input/google-research-identify-contrails-reduce-global-warming/{parrent_folder}')})\n",
    "        self.parrent_folder: str = parrent_folder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_idx)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id: str = str(self.df_idx.iloc[idx]['idx'])\n",
    "        images = torch.tensor(get_ash_color_images(image_id, self.parrent_folder, get_mask_frame_only=False)).to(torch.float32).permute(2, 3, 0, 1)\n",
    "        mask = torch.tensor(get_mask_image(image_id, self.parrent_folder)).to(torch.float32).permute(2, 0, 1)\n",
    "        return images, mask"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.205253,
     "end_time": "2023-05-31T10:17:45.669652",
     "exception": false,
     "start_time": "2023-05-31T10:17:45.464399",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-06-04T22:29:20.263374Z",
     "iopub.execute_input": "2023-06-04T22:29:20.264186Z",
     "iopub.status.idle": "2023-06-04T22:29:20.306325Z",
     "shell.execute_reply.started": "2023-06-04T22:29:20.264153Z",
     "shell.execute_reply": "2023-06-04T22:29:20.304731Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_train = ContrailsAshDataset('train')\n",
    "dataset_validation = ContrailsAshDataset('validation')\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=16, shuffle=True, num_workers=2)\n",
    "data_loader_validation = DataLoader(dataset_validation, batch_size=16, shuffle=True, num_workers=2)"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.220594,
     "end_time": "2023-05-31T10:17:46.075484",
     "exception": false,
     "start_time": "2023-05-31T10:17:45.854890",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-06-04T22:29:20.309096Z",
     "iopub.execute_input": "2023-06-04T22:29:20.309728Z",
     "iopub.status.idle": "2023-06-04T22:29:20.368116Z",
     "shell.execute_reply.started": "2023-06-04T22:29:20.309704Z",
     "shell.execute_reply": "2023-06-04T22:29:20.367040Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.191252,
     "end_time": "2023-05-31T10:17:46.453268",
     "exception": false,
     "start_time": "2023-05-31T10:17:46.262016",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train = True"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.200528,
     "end_time": "2023-05-31T10:17:46.840869",
     "exception": false,
     "start_time": "2023-05-31T10:17:46.640341",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-06-04T22:29:20.369972Z",
     "iopub.execute_input": "2023-06-04T22:29:20.370767Z",
     "iopub.status.idle": "2023-06-04T22:29:20.376187Z",
     "shell.execute_reply.started": "2023-06-04T22:29:20.370734Z",
     "shell.execute_reply": "2023-06-04T22:29:20.374991Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if train:\n",
    "    model = TestNet()\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(100))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.70)\n",
    "\n",
    "    num_epochs = 11\n",
    "\n",
    "    trainer = MyTrainer(model, optimizer, criterion, lr_scheduler)\n",
    "    trainer.fit(data_loader_train, data_loader_validation, epochs=num_epochs)\n",
    "else:\n",
    "    model = UNet()\n",
    "    model.load_state_dict(torch.load('/kaggle/input/contrails-unet-pretrained/unet.pt'))\n",
    "    model.eval()\n",
    "    model.to(device)"
   ],
   "metadata": {
    "papermill": {
     "duration": 13176.449641,
     "end_time": "2023-05-31T13:57:23.476998",
     "exception": false,
     "start_time": "2023-05-31T10:17:47.027357",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-06-04T22:29:20.378715Z",
     "iopub.execute_input": "2023-06-04T22:29:20.379078Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "New learning rate: [0.01]\nepotch: 0 batch: 0/1284 loss: nan\nepotch: 0 batch: 100/1284 loss: 0.9671872854232788\nepotch: 0 batch: 200/1284 loss: 0.7521615028381348\nepotch: 0 batch: 300/1284 loss: 0.6684869527816772\nepotch: 0 batch: 400/1284 loss: 0.6030632257461548\nepotch: 0 batch: 500/1284 loss: 0.6022924184799194\nepotch: 0 batch: 600/1284 loss: 0.5416792035102844\nepotch: 0 batch: 700/1284 loss: 0.5055953860282898\nepotch: 0 batch: 800/1284 loss: 0.5307925343513489\nepotch: 0 batch: 900/1284 loss: 0.5038981437683105\nepotch: 0 batch: 1000/1284 loss: 0.4985625147819519\nepotch: 0 batch: 1100/1284 loss: 0.48227035999298096\nepotch: 0 batch: 1200/1284 loss: 0.4428391754627228\nTrain Epoch: 0 Average Loss: 0.582951\nValidation loss after 1 epochs was 0.3229\nNew learning rate: [0.006999999999999999]\nepotch: 1 batch: 0/1284 loss: nan\nepotch: 1 batch: 100/1284 loss: 0.42149949073791504\nepotch: 1 batch: 200/1284 loss: 0.4134237766265869\nepotch: 1 batch: 300/1284 loss: 0.4066868722438812\nepotch: 1 batch: 400/1284 loss: 0.4040891230106354\nepotch: 1 batch: 500/1284 loss: 0.40765416622161865\nepotch: 1 batch: 600/1284 loss: 0.4002296030521393\nepotch: 1 batch: 700/1284 loss: 0.3982396721839905\nepotch: 1 batch: 800/1284 loss: 0.38586071133613586\nepotch: 1 batch: 900/1284 loss: 0.3507711887359619\nepotch: 1 batch: 1000/1284 loss: 0.36994311213493347\nepotch: 1 batch: 1100/1284 loss: 0.36468833684921265\nepotch: 1 batch: 1200/1284 loss: 0.3441348671913147\nTrain Epoch: 1 Average Loss: 0.386568\nValidation loss after 2 epochs was 0.2104\nNew learning rate: [0.004899999999999999]\nepotch: 2 batch: 0/1284 loss: nan\nepotch: 2 batch: 100/1284 loss: 0.3213890492916107\nepotch: 2 batch: 200/1284 loss: 0.3355497121810913\nepotch: 2 batch: 300/1284 loss: 0.3212061822414398\nepotch: 2 batch: 400/1284 loss: 0.2926897406578064\nepotch: 2 batch: 500/1284 loss: 0.2990535795688629\nepotch: 2 batch: 600/1284 loss: 0.3114522099494934\nepotch: 2 batch: 700/1284 loss: 0.27618035674095154\nepotch: 2 batch: 800/1284 loss: 0.25860869884490967\nepotch: 2 batch: 900/1284 loss: 0.31441015005111694\nepotch: 2 batch: 1000/1284 loss: 0.29827749729156494\nepotch: 2 batch: 1100/1284 loss: 0.2936636507511139\nepotch: 2 batch: 1200/1284 loss: 0.2923763394355774\nTrain Epoch: 2 Average Loss: 0.297957\nValidation loss after 3 epochs was 0.1708\nNew learning rate: [0.003429999999999999]\nepotch: 3 batch: 0/1284 loss: nan\nepotch: 3 batch: 100/1284 loss: 0.2570875287055969\nepotch: 3 batch: 200/1284 loss: 0.24308116734027863\nepotch: 3 batch: 300/1284 loss: 0.25642329454421997\nepotch: 3 batch: 400/1284 loss: 0.23276150226593018\nepotch: 3 batch: 500/1284 loss: 0.23626314103603363\nepotch: 3 batch: 600/1284 loss: 0.24398933351039886\nepotch: 3 batch: 700/1284 loss: 0.23764528334140778\nepotch: 3 batch: 800/1284 loss: 0.2304629534482956\nepotch: 3 batch: 900/1284 loss: 0.23830631375312805\nepotch: 3 batch: 1000/1284 loss: 0.23247762024402618\nepotch: 3 batch: 1100/1284 loss: 0.22586427628993988\nepotch: 3 batch: 1200/1284 loss: 0.21544687449932098\nTrain Epoch: 3 Average Loss: 0.236812\nValidation loss after 4 epochs was 0.1332\nNew learning rate: [0.002400999999999999]\nepotch: 4 batch: 0/1284 loss: nan\nepotch: 4 batch: 100/1284 loss: 0.2176068127155304\nepotch: 4 batch: 200/1284 loss: 0.19776315987110138\nepotch: 4 batch: 300/1284 loss: 0.20168472826480865\nepotch: 4 batch: 400/1284 loss: 0.20921756327152252\nepotch: 4 batch: 500/1284 loss: 0.20107220113277435\nepotch: 4 batch: 600/1284 loss: 0.20245486497879028\nepotch: 4 batch: 700/1284 loss: 0.2026209831237793\nepotch: 4 batch: 800/1284 loss: 0.19449329376220703\nepotch: 4 batch: 900/1284 loss: 0.20030342042446136\nepotch: 4 batch: 1000/1284 loss: 0.19615577161312103\nepotch: 4 batch: 1100/1284 loss: 0.19353070855140686\nepotch: 4 batch: 1200/1284 loss: 0.18060922622680664\nTrain Epoch: 4 Average Loss: 0.199455\nValidation loss after 5 epochs was 0.1373\nNew learning rate: [0.0016806999999999992]\nepotch: 5 batch: 0/1284 loss: nan\nepotch: 5 batch: 100/1284 loss: 0.18202324211597443\nepotch: 5 batch: 200/1284 loss: 0.17922405898571014\nepotch: 5 batch: 300/1284 loss: 0.16821561753749847\nepotch: 5 batch: 400/1284 loss: 0.18958841264247894\nepotch: 5 batch: 500/1284 loss: 0.19142553210258484\nepotch: 5 batch: 600/1284 loss: 0.1639113426208496\nepotch: 5 batch: 700/1284 loss: 0.1644866168498993\nepotch: 5 batch: 800/1284 loss: 0.17287540435791016\nepotch: 5 batch: 900/1284 loss: 0.18444545567035675\nepotch: 5 batch: 1000/1284 loss: 0.18592621386051178\nepotch: 5 batch: 1100/1284 loss: 0.18871019780635834\nepotch: 5 batch: 1200/1284 loss: 0.16573095321655273\nTrain Epoch: 5 Average Loss: 0.179008\nValidation loss after 6 epochs was 0.1165\nNew learning rate: [0.0011764899999999994]\nepotch: 6 batch: 0/1284 loss: nan\nepotch: 6 batch: 100/1284 loss: 0.17630250751972198\nepotch: 6 batch: 200/1284 loss: 0.1708926409482956\nepotch: 6 batch: 300/1284 loss: 0.17831557989120483\nepotch: 6 batch: 400/1284 loss: 0.16415446996688843\nepotch: 6 batch: 500/1284 loss: 0.1623111516237259\nepotch: 6 batch: 600/1284 loss: 0.1711658239364624\nepotch: 6 batch: 700/1284 loss: 0.1708364337682724\nepotch: 6 batch: 800/1284 loss: 0.1688530594110489\nepotch: 6 batch: 900/1284 loss: 0.1558111310005188\nepotch: 6 batch: 1000/1284 loss: 0.15133598446846008\nepotch: 6 batch: 1100/1284 loss: 0.15621793270111084\nepotch: 6 batch: 1200/1284 loss: 0.16309043765068054\nTrain Epoch: 6 Average Loss: 0.165354\nValidation loss after 7 epochs was 0.1042\nNew learning rate: [0.0008235429999999996]\nepotch: 7 batch: 0/1284 loss: nan\nepotch: 7 batch: 100/1284 loss: 0.14891719818115234\nepotch: 7 batch: 200/1284 loss: 0.1600121557712555\nepotch: 7 batch: 300/1284 loss: 0.15924085676670074\nepotch: 7 batch: 400/1284 loss: 0.15057045221328735\nepotch: 7 batch: 500/1284 loss: 0.14805908501148224\nepotch: 7 batch: 600/1284 loss: 0.15840870141983032\nepotch: 7 batch: 700/1284 loss: 0.149946391582489\nepotch: 7 batch: 800/1284 loss: 0.15316419303417206\nepotch: 7 batch: 900/1284 loss: 0.14990678429603577\nepotch: 7 batch: 1000/1284 loss: 0.1473851054906845\nepotch: 7 batch: 1100/1284 loss: 0.151414692401886\nepotch: 7 batch: 1200/1284 loss: 0.15380041301250458\nTrain Epoch: 7 Average Loss: 0.153334\nValidation loss after 8 epochs was 0.1017\nNew learning rate: [0.0005764800999999997]\nepotch: 8 batch: 0/1284 loss: nan\nepotch: 8 batch: 100/1284 loss: 0.1404491513967514\nepotch: 8 batch: 200/1284 loss: 0.15595366060733795\nepotch: 8 batch: 300/1284 loss: 0.14502373337745667\nepotch: 8 batch: 400/1284 loss: 0.1472691297531128\nepotch: 8 batch: 500/1284 loss: 0.14040468633174896\nepotch: 8 batch: 600/1284 loss: 0.1511656641960144\nepotch: 8 batch: 700/1284 loss: 0.14869330823421478\nepotch: 8 batch: 800/1284 loss: 0.1339636594057083\nepotch: 8 batch: 900/1284 loss: 0.15116997063159943\nepotch: 8 batch: 1000/1284 loss: 0.13819530606269836\nepotch: 8 batch: 1100/1284 loss: 0.1473400890827179\nepotch: 8 batch: 1200/1284 loss: 0.13218967616558075\nTrain Epoch: 8 Average Loss: 0.144520\nValidation loss after 9 epochs was 0.094\nNew learning rate: [0.00040353606999999974]\nepotch: 9 batch: 0/1284 loss: nan\nepotch: 9 batch: 100/1284 loss: 0.13162606954574585\nepotch: 9 batch: 200/1284 loss: 0.13738225400447845\nepotch: 9 batch: 300/1284 loss: 0.1382313072681427\nepotch: 9 batch: 400/1284 loss: 0.13806971907615662\nepotch: 9 batch: 500/1284 loss: 0.13669353723526\nepotch: 9 batch: 600/1284 loss: 0.14044953882694244\nepotch: 9 batch: 700/1284 loss: 0.13386335968971252\nepotch: 9 batch: 800/1284 loss: 0.14689934253692627\nepotch: 9 batch: 900/1284 loss: 0.14554481208324432\nepotch: 9 batch: 1000/1284 loss: 0.1340104341506958\nepotch: 9 batch: 1100/1284 loss: 0.1373378038406372\nepotch: 9 batch: 1200/1284 loss: 0.13874979317188263\nTrain Epoch: 9 Average Loss: 0.138080\nValidation loss after 10 epochs was 0.0921\nNew learning rate: [0.0002824752489999998]\nepotch: 10 batch: 0/1284 loss: nan\nepotch: 10 batch: 100/1284 loss: 0.13186609745025635\nepotch: 10 batch: 200/1284 loss: 0.13162966072559357\nepotch: 10 batch: 300/1284 loss: 0.1433047652244568\nepotch: 10 batch: 400/1284 loss: 0.1251073181629181\nepotch: 10 batch: 500/1284 loss: 0.12632901966571808\nepotch: 10 batch: 600/1284 loss: 0.12839728593826294\nepotch: 10 batch: 700/1284 loss: 0.13473781943321228\nepotch: 10 batch: 800/1284 loss: 0.12891560792922974\nepotch: 10 batch: 900/1284 loss: 0.1320599615573883\nepotch: 10 batch: 1000/1284 loss: 0.13002270460128784\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Overview"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.200641,
     "end_time": "2023-05-31T13:57:23.885502",
     "exception": false,
     "start_time": "2023-05-31T13:57:23.684861",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if train:\n",
    "    df_data = pd.DataFrame({'Batch Losses': trainer.batch_losses})\n",
    "\n",
    "    sns.lineplot(data=df_data)\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Batch Loss')\n",
    "    plt.show()"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.843086,
     "end_time": "2023-05-31T13:57:24.933941",
     "exception": false,
     "start_time": "2023-05-31T13:57:24.090855",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if train:\n",
    "    df_data = pd.DataFrame({'Loss': trainer.epoch_losses})\n",
    "\n",
    "    sns.lineplot(data=df_data)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Model Argavgre Training Loss over Epochs')\n",
    "    plt.show()"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.614575,
     "end_time": "2023-05-31T13:57:25.750235",
     "exception": false,
     "start_time": "2023-05-31T13:57:25.135660",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if train:\n",
    "    df_data = pd.DataFrame({'Loss': trainer.validation_losses})\n",
    "\n",
    "    sns.lineplot(data=df_data)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Model Validation Loss over Epochs')\n",
    "    plt.show()"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.533914,
     "end_time": "2023-05-31T13:57:26.488093",
     "exception": false,
     "start_time": "2023-05-31T13:57:25.954179",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if train:\n",
    "    df_data = pd.DataFrame({'Learning rates': trainer.learning_rates})\n",
    "\n",
    "    sns.lineplot(data=df_data)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learinig Rate')\n",
    "    plt.title('Learinig Rate over Epochs')\n",
    "    plt.show()"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.514957,
     "end_time": "2023-05-31T13:57:27.205759",
     "exception": false,
     "start_time": "2023-05-31T13:57:26.690802",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find Optimal Threshold"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.208245,
     "end_time": "2023-05-31T13:57:27.617417",
     "exception": false,
     "start_time": "2023-05-31T13:57:27.409172",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class DiceThresholdTester:\n",
    "    \n",
    "    def __init__(self, model: nn.Module, data_loader: torch.utils.data.DataLoader):\n",
    "        self.model = model\n",
    "        self.data_loader = data_loader\n",
    "        self.cumulative_mask_pred = []\n",
    "        self.cumulative_mask_true = []\n",
    "        \n",
    "    def precalculate_prediction(self) -> None:\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        for images, mask_true in self.data_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "\n",
    "            mask_pred = sigmoid(model.forward(images))\n",
    "\n",
    "            self.cumulative_mask_pred.append(mask_pred.cpu().detach().numpy())\n",
    "            self.cumulative_mask_true.append(mask_true.cpu().detach().numpy())\n",
    "            \n",
    "        self.cumulative_mask_pred = np.concatenate(self.cumulative_mask_pred, axis=0)\n",
    "        self.cumulative_mask_true = np.concatenate(self.cumulative_mask_true, axis=0)\n",
    "\n",
    "        self.cumulative_mask_pred = torch.flatten(torch.from_numpy(self.cumulative_mask_pred))\n",
    "        self.cumulative_mask_true = torch.flatten(torch.from_numpy(self.cumulative_mask_true))\n",
    "    \n",
    "    def test_threshold(self, threshold: float) -> float:\n",
    "        _dice = Dice(use_sigmoid=False)\n",
    "        after_threshold = np.zeros(self.cumulative_mask_pred.shape)\n",
    "        after_threshold[self.cumulative_mask_pred[:] > threshold] = 1\n",
    "        after_threshold[self.cumulative_mask_pred[:] < threshold] = 0\n",
    "        after_threshold = torch.flatten(torch.from_numpy(after_threshold))\n",
    "        return _dice(self.cumulative_mask_true, after_threshold).item()"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.230357,
     "end_time": "2023-05-31T13:57:28.056337",
     "exception": false,
     "start_time": "2023-05-31T13:57:27.825980",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dice_threshold_tester = DiceThresholdTester(model, data_loader_validation)\n",
    "dice_threshold_tester.precalculate_prediction()"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 103.416246,
     "end_time": "2023-05-31T13:59:11.678648",
     "exception": false,
     "start_time": "2023-05-31T13:57:28.262402",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "thresholds_to_test = [round(x * 0.01, 2) for x in range(101)]\n",
    "\n",
    "optim_threshold = 0.975\n",
    "best_dice_score = -1\n",
    "\n",
    "thresholds = []\n",
    "dice_scores = []\n",
    "\n",
    "for t in thresholds_to_test:\n",
    "    dice_score = dice_threshold_tester.test_threshold(t)\n",
    "    if dice_score > best_dice_score:\n",
    "        best_dice_score = dice_score\n",
    "        optim_threshold = t\n",
    "    \n",
    "    thresholds.append(t)\n",
    "    dice_scores.append(dice_score)\n",
    "    \n",
    "print(f'Best Threshold: {optim_threshold} with dice: {best_dice_score}')\n",
    "df_threshold_data = pd.DataFrame({'Threshold': thresholds, 'Dice Score': dice_scores})"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 313.139354,
     "end_time": "2023-05-31T14:04:25.070902",
     "exception": false,
     "start_time": "2023-05-31T13:59:11.931548",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sns.lineplot(data=df_threshold_data, x='Threshold', y='Dice Score')\n",
    "plt.axhline(y=best_dice_score, color='green')\n",
    "plt.axvline(x=optim_threshold, color='green')\n",
    "plt.text(-0.02, best_dice_score * 0.96, f'{best_dice_score:.3f}', va='center', ha='left', color='green')\n",
    "plt.text(optim_threshold - 0.01, 0.02, f'{optim_threshold}', va='center', ha='right', color='green')\n",
    "plt.ylim(bottom=0)\n",
    "plt.title('Threshold vs Dice Score')\n",
    "plt.show()"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.546596,
     "end_time": "2023-05-31T14:04:25.821157",
     "exception": false,
     "start_time": "2023-05-31T14:04:25.274561",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preview Models Predictions on Validation"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.209061,
     "end_time": "2023-05-31T14:04:26.234717",
     "exception": false,
     "start_time": "2023-05-31T14:04:26.025656",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "batches_to_show = 4\n",
    "model.eval()\n",
    "\n",
    "for i, data in enumerate(data_loader_validation):\n",
    "    images, mask = data\n",
    "    \n",
    "    # Predict mask for this instance\n",
    "    if torch.cuda.is_available():\n",
    "        images = images.cuda()\n",
    "    predicated_mask = sigmoid(model.forward(images[:, :, :, :]).cpu().detach().numpy())\n",
    "    \n",
    "    # Apply threshold\n",
    "    predicated_mask_with_threshold = np.zeros((images.shape[0], 256, 256))\n",
    "    predicated_mask_with_threshold[predicated_mask[:, 0, :, :] < optim_threshold] = 0\n",
    "    predicated_mask_with_threshold[predicated_mask[:, 0, :, :] > optim_threshold] = 1\n",
    "    \n",
    "    images = images.cpu()\n",
    "        \n",
    "    for img_num in range(0, images.shape[0]):\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20,10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Show groud trought \n",
    "        axes[0].imshow(mask[img_num, 0, :, :])\n",
    "        axes[0].axis('off')\n",
    "        axes[0].set_title('Ground Truth')\n",
    "        \n",
    "        # Show ash color scheme input image\n",
    "        axes[1].imshow( np.concatenate(\n",
    "            (\n",
    "            np.expand_dims(images[img_num, 4, :, :], axis=2),\n",
    "            np.expand_dims(images[img_num, 12, :, :], axis=2),\n",
    "            np.expand_dims(images[img_num, 20, :, :], axis=2)\n",
    "        ), axis=2))\n",
    "        axes[1].axis('off')\n",
    "        axes[1].set_title('Ash color scheeme input - Frame 4')\n",
    "\n",
    "        # Show predicted mask\n",
    "        axes[2].imshow(predicated_mask[img_num, 0, :, :], vmin=0, vmax=1)\n",
    "        axes[2].axis('off')\n",
    "        axes[2].set_title('Predicted probability mask')\n",
    "\n",
    "        # Show predicted mask after threshold\n",
    "        axes[3].imshow(predicated_mask_with_threshold[img_num, :, :])\n",
    "        axes[3].axis('off')\n",
    "        axes[3].set_title('Predicted mask with threshold')\n",
    "        plt.show()\n",
    "    \n",
    "    if i + 1 >= batches_to_show:\n",
    "        break"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 41.858322,
     "end_time": "2023-05-31T14:05:08.301311",
     "exception": false,
     "start_time": "2023-05-31T14:04:26.442989",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Submission"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.436969,
     "end_time": "2023-05-31T14:05:09.190052",
     "exception": false,
     "start_time": "2023-05-31T14:05:08.753083",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Same a the ContrailsAshDataset but does not load the mask (since its not avalble for test) and returns image_id instaed of the mask to assamble submission\n",
    "class ContrailsAshTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.df_idx: pd.DataFrame = pd.DataFrame({'idx': os.listdir(f'/kaggle/input/google-research-identify-contrails-reduce-global-warming/test')})\n",
    "        self.parrent_folder: str = 'test'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_idx)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id: int = int(self.df_idx.iloc[idx]['idx'])\n",
    "        images = torch.tensor(np.reshape(get_ash_color_images(str(image_id), self.parrent_folder, get_mask_frame_only=False), (256, 256, 24))).to(torch.float32).permute(2, 0, 1)\n",
    "        return images,  torch.tensor(image_id)"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.471801,
     "end_time": "2023-05-31T14:05:10.110001",
     "exception": false,
     "start_time": "2023-05-31T14:05:09.638200",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_test = ContrailsAshTestDataset()\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=16, shuffle=True, num_workers=2)"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.458884,
     "end_time": "2023-05-31T14:05:11.017444",
     "exception": false,
     "start_time": "2023-05-31T14:05:10.558560",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#source https://www.kaggle.com/code/inversion/contrails-rle-submission?scriptVersionId=128527711&cellId=4\n",
    "\n",
    "def rle_encode(x, fg_val=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x:  numpy array of shape (height, width), 1 - mask, 0 - background\n",
    "    Returns: run length encoding as list\n",
    "    \"\"\"\n",
    "\n",
    "    dots = np.where(\n",
    "        x.T.flatten() == fg_val)[0]  # .T sets Fortran order down-then-right\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if b > prev + 1:\n",
    "            run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "\n",
    "def list_to_string(x):\n",
    "    \"\"\"\n",
    "    Converts list to a string representation\n",
    "    Empty list returns '-'\n",
    "    \"\"\"\n",
    "    if x: # non-empty list\n",
    "        s = str(x).replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\")\n",
    "    else:\n",
    "        s = '-'\n",
    "    return s"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.457623,
     "end_time": "2023-05-31T14:05:11.928794",
     "exception": false,
     "start_time": "2023-05-31T14:05:11.471171",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "submission = pd.read_csv('/kaggle/input/google-research-identify-contrails-reduce-global-warming/sample_submission.csv', index_col='record_id')"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.612617,
     "end_time": "2023-05-31T14:05:12.990868",
     "exception": false,
     "start_time": "2023-05-31T14:05:12.378251",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i, data in enumerate(data_loader_test):\n",
    "    images, image_id = data\n",
    "    \n",
    "    # Predict mask for this instance\n",
    "    if torch.cuda.is_available():\n",
    "        images = images.cuda()\n",
    "    predicated_mask = sigmoid(model.forward(images[:, :, :, :]).cpu().detach().numpy())\n",
    "    \n",
    "    # Apply threshold\n",
    "    predicated_mask_with_threshold = np.zeros((images.shape[0], 256, 256))\n",
    "    predicated_mask_with_threshold[predicated_mask[:, 0, :, :] < optim_threshold] = 0\n",
    "    predicated_mask_with_threshold[predicated_mask[:, 0, :, :] > optim_threshold] = 1\n",
    "    \n",
    "    for img_num in range(0, images.shape[0]):\n",
    "        current_mask = predicated_mask_with_threshold[img_num, :, :]\n",
    "        current_image_id = image_id[img_num].item()\n",
    "        \n",
    "        submission.loc[int(current_image_id), 'encoded_pixels'] = list_to_string(rle_encode(current_mask))"
   ],
   "metadata": {
    "papermill": {
     "duration": 2.222545,
     "end_time": "2023-05-31T14:05:15.663330",
     "exception": false,
     "start_time": "2023-05-31T14:05:13.440785",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "submission"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.480585,
     "end_time": "2023-05-31T14:05:16.593303",
     "exception": false,
     "start_time": "2023-05-31T14:05:16.112718",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "submission.to_csv('submission.csv')"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.465037,
     "end_time": "2023-05-31T14:05:17.519780",
     "exception": false,
     "start_time": "2023-05-31T14:05:17.054743",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
