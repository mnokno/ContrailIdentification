{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from matplotlib import animation\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "import torchvision.models as models\n",
    "\n",
    "!pip install torchsummary\n",
    "from torchsummary import summary"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-05T15:09:53.837492Z",
     "iopub.execute_input": "2023-06-05T15:09:53.838165Z",
     "iopub.status.idle": "2023-06-05T15:10:09.828092Z",
     "shell.execute_reply.started": "2023-06-05T15:09:53.838103Z",
     "shell.execute_reply": "2023-06-05T15:10:09.826929Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utilites"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "data_dir: str = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-05T15:10:09.832411Z",
     "iopub.execute_input": "2023-06-05T15:10:09.834230Z",
     "iopub.status.idle": "2023-06-05T15:10:09.839085Z",
     "shell.execute_reply.started": "2023-06-05T15:10:09.834191Z",
     "shell.execute_reply": "2023-06-05T15:10:09.838086Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-05T15:10:09.840482Z",
     "iopub.execute_input": "2023-06-05T15:10:09.840886Z",
     "iopub.status.idle": "2023-06-05T15:10:09.883288Z",
     "shell.execute_reply.started": "2023-06-05T15:10:09.840852Z",
     "shell.execute_reply": "2023-06-05T15:10:09.882315Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "execution_count": 3,
     "output_type": "execute_result",
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_band_images(idx: str, parrent_folder: str, band: str) -> np.array:\n",
    "    return np.load(os.path.join(data_dir, parrent_folder, idx, f'band_{band}.npy'))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-05T15:10:09.886007Z",
     "iopub.execute_input": "2023-06-05T15:10:09.886713Z",
     "iopub.status.idle": "2023-06-05T15:10:09.892644Z",
     "shell.execute_reply.started": "2023-06-05T15:10:09.886681Z",
     "shell.execute_reply": "2023-06-05T15:10:09.891703Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "_T11_BOUNDS = (243, 303)\n",
    "_CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n",
    "_TDIFF_BOUNDS = (-4, 2)\n",
    "\n",
    "def normalize_range(data, bounds):\n",
    "    \"\"\"Maps data to the range [0, 1].\"\"\"\n",
    "    return (data - bounds[0]) / (bounds[1] - bounds[0])\n",
    "\n",
    "\n",
    "def get_ash_color_images(idx: str, parrent_folder: str, get_mask_frame_only=False) -> np.array:\n",
    "    band11 = get_band_images(idx, parrent_folder, '11')\n",
    "    band14 = get_band_images(idx, parrent_folder, '14')\n",
    "    band15 = get_band_images(idx, parrent_folder, '15')\n",
    "    \n",
    "    if get_mask_frame_only:\n",
    "        band11 = band11[:,:,4]\n",
    "        band14 = band14[:,:,4]\n",
    "        band15 = band15[:,:,4]\n",
    "\n",
    "    r = normalize_range(band15 - band14, _TDIFF_BOUNDS)\n",
    "    g = normalize_range(band14 - band11, _CLOUD_TOP_TDIFF_BOUNDS)\n",
    "    b = normalize_range(band14, _T11_BOUNDS)\n",
    "    false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n",
    "    return false_color"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-05T15:10:09.894093Z",
     "iopub.execute_input": "2023-06-05T15:10:09.894759Z",
     "iopub.status.idle": "2023-06-05T15:10:09.904748Z",
     "shell.execute_reply.started": "2023-06-05T15:10:09.894729Z",
     "shell.execute_reply": "2023-06-05T15:10:09.903882Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_mask_image(idx: str, parrent_folder: str) -> np.array:\n",
    "    return np.load(os.path.join(data_dir, parrent_folder, idx, 'human_pixel_masks.npy')) "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-05T15:10:09.906115Z",
     "iopub.execute_input": "2023-06-05T15:10:09.906691Z",
     "iopub.status.idle": "2023-06-05T15:10:09.915811Z",
     "shell.execute_reply.started": "2023-06-05T15:10:09.906661Z",
     "shell.execute_reply": "2023-06-05T15:10:09.915067Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class Upsampled(nn.Module):\n",
    "    def __init__(self, back_bone: nn.Module):\n",
    "        super(Upsampled, self).__init__()\n",
    "        self.up = nn.Upsample(scale_factor=1.75, mode='bilinear', align_corners=False)\n",
    "        self.back_bone = back_bone\n",
    "        self.down = nn.Upsample(scale_factor=0.572, mode='bilinear', align_corners=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the layers\n",
    "        x = self.up(x)\n",
    "        x = self.back_bone(x)['out']\n",
    "        x = self.down(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_no_downsample_back(self, x):\n",
    "        # Forward pass through the layers\n",
    "        x = self.up(x)\n",
    "        x = self.back_bone(x)['out']\n",
    "        return x "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-05T15:10:09.917071Z",
     "iopub.execute_input": "2023-06-05T15:10:09.917640Z",
     "iopub.status.idle": "2023-06-05T15:10:09.926689Z",
     "shell.execute_reply.started": "2023-06-05T15:10:09.917610Z",
     "shell.execute_reply": "2023-06-05T15:10:09.925910Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "back_bone = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)\n",
    "back_bone.backbone.conv1 = nn.Conv2d(24, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "back_bone.aux_classifier[4] = nn.Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
    "back_bone.classifier[4] = nn.Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "model = Upsampled(back_bone)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-05T15:10:09.927898Z",
     "iopub.execute_input": "2023-06-05T15:10:09.928484Z",
     "iopub.status.idle": "2023-06-05T15:10:14.416220Z",
     "shell.execute_reply.started": "2023-06-05T15:10:09.928455Z",
     "shell.execute_reply": "2023-06-05T15:10:14.415192Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "text": "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet50_coco-cd0a2569.pth\n100%|██████████| 161M/161M [00:00<00:00, 241MB/s] \n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#model.load_state_dict(torch.load('/kaggle/working/model_checkpoint_e1.pt'))\n",
    "#model.eval()\n",
    "#model.to(device)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-05T15:10:14.417782Z",
     "iopub.execute_input": "2023-06-05T15:10:14.418156Z",
     "iopub.status.idle": "2023-06-05T15:10:14.422885Z",
     "shell.execute_reply.started": "2023-06-05T15:10:14.418124Z",
     "shell.execute_reply": "2023-06-05T15:10:14.422000Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.to(device)\n",
    "model"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-05T15:10:14.427304Z",
     "iopub.execute_input": "2023-06-05T15:10:14.428060Z",
     "iopub.status.idle": "2023-06-05T15:10:17.235785Z",
     "shell.execute_reply.started": "2023-06-05T15:10:14.428028Z",
     "shell.execute_reply": "2023-06-05T15:10:17.234857Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "execution_count": 10,
     "output_type": "execute_result",
     "data": {
      "text/plain": "Upsampled(\n  (up): Upsample(scale_factor=1.75, mode='bilinear')\n  (back_bone): DeepLabV3(\n    (backbone): IntermediateLayerGetter(\n      (conv1): Conv2d(24, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (classifier): DeepLabHead(\n      (0): ASPP(\n        (convs): ModuleList(\n          (0): Sequential(\n            (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU()\n          )\n          (1): ASPPConv(\n            (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU()\n          )\n          (2): ASPPConv(\n            (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU()\n          )\n          (3): ASPPConv(\n            (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU()\n          )\n          (4): ASPPPooling(\n            (0): AdaptiveAvgPool2d(output_size=1)\n            (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (3): ReLU()\n          )\n        )\n        (project): Sequential(\n          (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU()\n          (3): Dropout(p=0.5, inplace=False)\n        )\n      )\n      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU()\n      (4): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (aux_classifier): FCNHead(\n      (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n      (3): Dropout(p=0.1, inplace=False)\n      (4): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (down): Upsample(scale_factor=0.572, mode='bilinear')\n)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#summary(model, input_size=(24, 256, 256))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-05T15:10:17.237391Z",
     "iopub.execute_input": "2023-06-05T15:10:17.238053Z",
     "iopub.status.idle": "2023-06-05T15:10:17.242228Z",
     "shell.execute_reply.started": "2023-06-05T15:10:17.238019Z",
     "shell.execute_reply": "2023-06-05T15:10:17.241331Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trainer"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-31T21:02:32.133730Z",
     "iopub.execute_input": "2023-05-31T21:02:32.134168Z",
     "iopub.status.idle": "2023-05-31T21:02:32.138968Z",
     "shell.execute_reply.started": "2023-05-31T21:02:32.134136Z",
     "shell.execute_reply": "2023-05-31T21:02:32.137968Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Dice(nn.Module):\n",
    "    def __init__(self, use_sigmoid=True):\n",
    "        super(Dice, self).__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.use_sigmoid = use_sigmoid\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        if self.use_sigmoid:\n",
    "            inputs = self.sigmoid(inputs)       \n",
    "        \n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2.0 *intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return dice\n",
    "    \n",
    "dice = Dice()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-05T15:10:17.243751Z",
     "iopub.execute_input": "2023-06-05T15:10:17.244375Z",
     "iopub.status.idle": "2023-06-05T15:10:17.253478Z",
     "shell.execute_reply.started": "2023-06-05T15:10:17.244342Z",
     "shell.execute_reply": "2023-06-05T15:10:17.252406Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class MyTrainer:\n",
    "    def __init__(self, model, optimizer, loss_fn, lr_scheduler):\n",
    "        self.validation_losses = []\n",
    "        self.batch_losses = []\n",
    "        self.epoch_losses = []\n",
    "        self.learning_rates = []\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self._check_optim_net_aligned()\n",
    "\n",
    "    # Ensures that the given optimizer points to the given model\n",
    "    def _check_optim_net_aligned(self):\n",
    "        assert self.optimizer.param_groups[0]['params'] == list(self.model.parameters())\n",
    "\n",
    "    # Trains the model\n",
    "    def fit(self,\n",
    "            train_dataloader: DataLoader,\n",
    "            test_dataloader: DataLoader,\n",
    "            epochs: int = 10,\n",
    "            eval_every: int = 1,\n",
    "            ):\n",
    "  \n",
    "        for e in range(epochs):\n",
    "            print(\"New learning rate: {}\".format(self.lr_scheduler.get_last_lr()))\n",
    "            self.learning_rates.append(self.lr_scheduler.get_last_lr()[0])\n",
    "\n",
    "            # Stores data about the batch\n",
    "            batch_losses = []\n",
    "            sub_batch_losses = []\n",
    "\n",
    "            for i, data in enumerate(train_dataloader):\n",
    "                self.model.train()\n",
    "                if i % 100 == 0:\n",
    "                    print(f'epotch: {e} batch: {i}/{len(train_dataloader)} loss: {torch.Tensor(sub_batch_losses).mean()}')\n",
    "                    sub_batch_losses.clear()\n",
    "                # Every data instance is an input + label pair\n",
    "                images, mask = data\n",
    "                \n",
    "                images = images.to(device)\n",
    "                mask = mask.to(device)\n",
    "\n",
    "                # Zero your gradients for every batch!\n",
    "                self.optimizer.zero_grad()\n",
    "                # Make predictions for this batch\n",
    "                outputs = self.model(images)#['out']\n",
    "                # Compute the loss and its gradients\n",
    "                loss = self.loss_fn(outputs, mask)\n",
    "                loss.backward()\n",
    "                # Adjust learning weights\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Saves data\n",
    "                self.batch_losses.append(loss.item())\n",
    "                batch_losses.append(loss)\n",
    "                sub_batch_losses.append(loss)\n",
    "\n",
    "            # Adjusts learning rate\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "\n",
    "            # Reports on the path\n",
    "            mean_epoch_loss = torch.Tensor(batch_losses).mean()\n",
    "            self.epoch_losses.append(mean_epoch_loss.item())\n",
    "            print('Train Epoch: {} Average Loss: {:.6f}'.format(e, mean_epoch_loss))\n",
    "\n",
    "            # Reports on the training progress\n",
    "            if (e + 1) % eval_every == 0:\n",
    "                torch.save(self.model.state_dict(), \"model_checkpoint_e\" + str(e) + \".pt\")\n",
    "                with torch.no_grad():\n",
    "                    self.model.eval()\n",
    "                    losses = []\n",
    "                    for i, data in enumerate(test_dataloader):\n",
    "                        # Every data instance is an input + label pair\n",
    "                        images, mask = data\n",
    "\n",
    "                        images = images.to(device)\n",
    "                        mask = mask.to(device)\n",
    "\n",
    "                        output = self.model(images)#['out']\n",
    "                        loss = self.loss_fn(output, mask)\n",
    "                        losses.append(loss.item())\n",
    "                        \n",
    "                    avg_loss = torch.Tensor(losses).mean().item()\n",
    "                    self.validation_losses.append(avg_loss)\n",
    "                    print(\"Validation loss after\", (e + 1), \"epochs was\", round(avg_loss, 4))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-05T15:10:17.255136Z",
     "iopub.execute_input": "2023-06-05T15:10:17.255484Z",
     "iopub.status.idle": "2023-06-05T15:10:17.274128Z",
     "shell.execute_reply.started": "2023-06-05T15:10:17.255454Z",
     "shell.execute_reply": "2023-06-05T15:10:17.273010Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class ContrailsAshDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, parrent_folder: str):\n",
    "        self.df_idx: pd.DataFrame = pd.DataFrame({'idx': os.listdir(f'/kaggle/input/google-research-identify-contrails-reduce-global-warming/{parrent_folder}')})\n",
    "        self.parrent_folder: str = parrent_folder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_idx)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id: str = str(self.df_idx.iloc[idx]['idx'])\n",
    "        images = torch.tensor(np.reshape(get_ash_color_images(image_id, self.parrent_folder, get_mask_frame_only=False), (256, 256, 24))).to(torch.float32).permute(2, 0, 1)\n",
    "        mask = torch.tensor(get_mask_image(image_id, self.parrent_folder)).to(torch.float32).permute(2, 0, 1)\n",
    "        return images, mask"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-05T15:10:17.275492Z",
     "iopub.execute_input": "2023-06-05T15:10:17.275939Z",
     "iopub.status.idle": "2023-06-05T15:10:17.288465Z",
     "shell.execute_reply.started": "2023-06-05T15:10:17.275907Z",
     "shell.execute_reply": "2023-06-05T15:10:17.287423Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_train = ContrailsAshDataset('train')\n",
    "dataset_validation = ContrailsAshDataset('validation')\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=16, shuffle=True, num_workers=2, drop_last=True)\n",
    "data_loader_validation = DataLoader(dataset_validation, batch_size=16, shuffle=True, num_workers=2, drop_last=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-05T15:10:17.290124Z",
     "iopub.execute_input": "2023-06-05T15:10:17.290460Z",
     "iopub.status.idle": "2023-06-05T15:10:17.696548Z",
     "shell.execute_reply.started": "2023-06-05T15:10:17.290430Z",
     "shell.execute_reply": "2023-06-05T15:10:17.695618Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "len(os.listdir(f'/kaggle/input/google-research-identify-contrails-reduce-global-warming/train'))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-05T15:10:17.699606Z",
     "iopub.execute_input": "2023-06-05T15:10:17.700577Z",
     "iopub.status.idle": "2023-06-05T15:10:17.715366Z",
     "shell.execute_reply.started": "2023-06-05T15:10:17.700542Z",
     "shell.execute_reply": "2023-06-05T15:10:17.714398Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": [
    {
     "execution_count": 16,
     "output_type": "execute_result",
     "data": {
      "text/plain": "20529"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(10))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.02)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, (2.0/3.0))\n",
    "model.train()\n",
    "\n",
    "num_epochs = 7\n",
    "\n",
    "trainer = MyTrainer(model, optimizer, criterion, lr_scheduler)\n",
    "trainer.fit(data_loader_train, data_loader_validation, epochs=num_epochs)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-05T15:10:17.716928Z",
     "iopub.execute_input": "2023-06-05T15:10:17.717283Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "New learning rate: [0.02]\nepotch: 0 batch: 0/1283 loss: nan\nepotch: 0 batch: 100/1283 loss: 0.23075006902217865\nepotch: 0 batch: 200/1283 loss: 0.199581578373909\nepotch: 0 batch: 300/1283 loss: 0.1863572746515274\nepotch: 0 batch: 400/1283 loss: 0.20983655750751495\nepotch: 0 batch: 500/1283 loss: 0.21037618815898895\nepotch: 0 batch: 600/1283 loss: 0.20533591508865356\nepotch: 0 batch: 700/1283 loss: 0.18869981169700623\nepotch: 0 batch: 800/1283 loss: 0.17548036575317383\nepotch: 0 batch: 900/1283 loss: 0.18315942585468292\nepotch: 0 batch: 1000/1283 loss: 0.1657150387763977\nepotch: 0 batch: 1100/1283 loss: 0.16209064424037933\nepotch: 0 batch: 1200/1283 loss: 0.14614972472190857\nTrain Epoch: 0 Average Loss: 0.185295\nValidation loss after 1 epochs was 0.0663\nNew learning rate: [0.013333333333333332]\nepotch: 1 batch: 0/1283 loss: nan\nepotch: 1 batch: 100/1283 loss: 0.1272886097431183\nepotch: 1 batch: 200/1283 loss: 0.11330203711986542\nepotch: 1 batch: 300/1283 loss: 0.1145106628537178\nepotch: 1 batch: 400/1283 loss: 0.11720962822437286\nepotch: 1 batch: 500/1283 loss: 0.11521074175834656\nepotch: 1 batch: 600/1283 loss: 0.10662295669317245\nepotch: 1 batch: 700/1283 loss: 0.10317131131887436\nepotch: 1 batch: 800/1283 loss: 0.09887835383415222\nepotch: 1 batch: 900/1283 loss: 0.0995064228773117\nepotch: 1 batch: 1000/1283 loss: 0.09599010646343231\nepotch: 1 batch: 1100/1283 loss: 0.09504993259906769\nepotch: 1 batch: 1200/1283 loss: 0.09532206505537033\nTrain Epoch: 1 Average Loss: 0.106333\nValidation loss after 2 epochs was 0.0502\nNew learning rate: [0.008888888888888887]\nepotch: 2 batch: 0/1283 loss: nan\nepotch: 2 batch: 100/1283 loss: 0.09128869324922562\nepotch: 2 batch: 200/1283 loss: 0.08853307366371155\nepotch: 2 batch: 300/1283 loss: 0.08979398757219315\nepotch: 2 batch: 400/1283 loss: 0.08997377753257751\nepotch: 2 batch: 500/1283 loss: 0.09229367971420288\nepotch: 2 batch: 600/1283 loss: 0.0808805525302887\nepotch: 2 batch: 700/1283 loss: 0.08408692479133606\nepotch: 2 batch: 800/1283 loss: 0.0882158949971199\nepotch: 2 batch: 900/1283 loss: 0.08033040165901184\nepotch: 2 batch: 1000/1283 loss: 0.08337248861789703\nepotch: 2 batch: 1100/1283 loss: 0.07688182592391968\nepotch: 2 batch: 1200/1283 loss: 0.08484245091676712\nTrain Epoch: 2 Average Loss: 0.085269\nValidation loss after 3 epochs was 0.0397\nNew learning rate: [0.005925925925925925]\nepotch: 3 batch: 0/1283 loss: nan\nepotch: 3 batch: 100/1283 loss: 0.07584848999977112\nepotch: 3 batch: 200/1283 loss: 0.07396552711725235\nepotch: 3 batch: 300/1283 loss: 0.0774863138794899\nepotch: 3 batch: 400/1283 loss: 0.06554674357175827\nepotch: 3 batch: 500/1283 loss: 0.07263755053281784\nepotch: 3 batch: 600/1283 loss: 0.07773737609386444\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find Optimal Threshold"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class DiceThresholdTester:\n",
    "    \n",
    "    def __init__(self, model: nn.Module, data_loader: torch.utils.data.DataLoader):\n",
    "        self.model = model\n",
    "        self.data_loader = data_loader\n",
    "        self.cumulative_mask_pred = []\n",
    "        self.cumulative_mask_true = []\n",
    "        \n",
    "    def precalculate_prediction(self) -> None:\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            sigmoid = nn.Sigmoid()\n",
    "\n",
    "            for images, mask_true in self.data_loader:\n",
    "                if torch.cuda.is_available():\n",
    "                    images = images.cuda()\n",
    "\n",
    "                mask_pred = sigmoid(model.forward(images))\n",
    "\n",
    "                self.cumulative_mask_pred.append(mask_pred.cpu().detach().numpy())\n",
    "                self.cumulative_mask_true.append(mask_true.cpu().detach().numpy())\n",
    "\n",
    "            self.cumulative_mask_pred = np.concatenate(self.cumulative_mask_pred, axis=0)\n",
    "            self.cumulative_mask_true = np.concatenate(self.cumulative_mask_true, axis=0)\n",
    "\n",
    "            self.cumulative_mask_pred = torch.flatten(torch.from_numpy(self.cumulative_mask_pred))\n",
    "            self.cumulative_mask_true = torch.flatten(torch.from_numpy(self.cumulative_mask_true))\n",
    "    \n",
    "    def test_threshold(self, threshold: float) -> float:\n",
    "        _dice = Dice(use_sigmoid=False)\n",
    "        after_threshold = np.zeros(self.cumulative_mask_pred.shape)\n",
    "        after_threshold[self.cumulative_mask_pred[:] > threshold] = 1\n",
    "        after_threshold[self.cumulative_mask_pred[:] < threshold] = 0\n",
    "        after_threshold = torch.flatten(torch.from_numpy(after_threshold))\n",
    "        return _dice(self.cumulative_mask_true, after_threshold).item()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dice_threshold_tester = DiceThresholdTester(model, data_loader_validation)\n",
    "dice_threshold_tester.precalculate_prediction()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "thresholds_to_test = [round(x * 0.01, 2) for x in range(101)]\n",
    "\n",
    "optim_threshold = 0.975\n",
    "best_dice_score = -1\n",
    "\n",
    "thresholds = []\n",
    "dice_scores = []\n",
    "\n",
    "for t in thresholds_to_test:\n",
    "    dice_score = dice_threshold_tester.test_threshold(t)\n",
    "    if dice_score > best_dice_score:\n",
    "        best_dice_score = dice_score\n",
    "        optim_threshold = t\n",
    "    \n",
    "    thresholds.append(t)\n",
    "    dice_scores.append(dice_score)\n",
    "    \n",
    "print(f'Best Threshold: {optim_threshold} with dice: {best_dice_score}')\n",
    "df_threshold_data = pd.DataFrame({'Threshold': thresholds, 'Dice Score': dice_scores})"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sns.lineplot(data=df_threshold_data, x='Threshold', y='Dice Score')\n",
    "plt.axhline(y=best_dice_score, color='green')\n",
    "plt.axvline(x=optim_threshold, color='green')\n",
    "plt.text(-0.02, best_dice_score * 0.96, f'{best_dice_score:.3f}', va='center', ha='left', color='green')\n",
    "plt.text(optim_threshold - 0.01, 0.02, f'{optim_threshold}', va='center', ha='right', color='green')\n",
    "plt.ylim(bottom=0)\n",
    "plt.title('Threshold vs Dice Score')\n",
    "plt.show()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "batches_to_show = 4\n",
    "model.eval()\n",
    "\n",
    "for i, data in enumerate(data_loader_validation):\n",
    "    images, mask = data\n",
    "    \n",
    "    # Predict mask for this instance\n",
    "    if torch.cuda.is_available():\n",
    "        images = images.cuda()\n",
    "    predicated_mask = sigmoid(model.forward(images[:, :, :, :]).cpu().detach().numpy())\n",
    "    \n",
    "    # Apply threshold\n",
    "    predicated_mask_with_threshold = np.zeros((images.shape[0], 256, 256))\n",
    "    predicated_mask_with_threshold[predicated_mask[:, 0, :, :] < optim_threshold] = 0\n",
    "    predicated_mask_with_threshold[predicated_mask[:, 0, :, :] > optim_threshold] = 1\n",
    "    \n",
    "    images = images.cpu()\n",
    "        \n",
    "    for img_num in range(0, images.shape[0]):\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20,10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Show groud trought \n",
    "        axes[0].imshow(mask[img_num, 0, :, :])\n",
    "        axes[0].axis('off')\n",
    "        axes[0].set_title('Ground Truth')\n",
    "        \n",
    "        # Show ash color scheme input image\n",
    "        axes[1].imshow( np.concatenate(\n",
    "            (\n",
    "            np.expand_dims(images[img_num, 4, :, :], axis=2),\n",
    "            np.expand_dims(images[img_num, 12, :, :], axis=2),\n",
    "            np.expand_dims(images[img_num, 20, :, :], axis=2)\n",
    "        ), axis=2))\n",
    "        axes[1].axis('off')\n",
    "        axes[1].set_title('Ash color scheeme input - Frame 4')\n",
    "\n",
    "        # Show predicted mask\n",
    "        axes[2].imshow(predicated_mask[img_num, 0, :, :], vmin=0, vmax=1)\n",
    "        axes[2].axis('off')\n",
    "        axes[2].set_title('Predicted probability mask')\n",
    "\n",
    "        # Show predicted mask after threshold\n",
    "        axes[3].imshow(predicated_mask_with_threshold[img_num, :, :])\n",
    "        axes[3].axis('off')\n",
    "        axes[3].set_title('Predicted mask with threshold')\n",
    "        plt.show()\n",
    "    \n",
    "    if i + 1 >= batches_to_show:\n",
    "        break"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
