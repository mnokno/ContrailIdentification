{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"!pip install -q torchsummary\n!pip install -q segmentation_models_pytorch","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:27:02.050369Z","iopub.execute_input":"2023-06-07T22:27:02.051140Z","iopub.status.idle":"2023-06-07T22:27:34.336567Z","shell.execute_reply.started":"2023-06-07T22:27:02.051102Z","shell.execute_reply":"2023-06-07T22:27:34.335247Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nfrom matplotlib import animation\nfrom IPython import display\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch import Tensor\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.models.detection import maskrcnn_resnet50_fpn\nimport torchvision.models as models\n\nfrom torchsummary import summary\nimport segmentation_models_pytorch as smp","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:27:34.341225Z","iopub.execute_input":"2023-06-07T22:27:34.341590Z","iopub.status.idle":"2023-06-07T22:27:40.778390Z","shell.execute_reply.started":"2023-06-07T22:27:34.341558Z","shell.execute_reply":"2023-06-07T22:27:40.777394Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Utilites","metadata":{}},{"cell_type":"code","source":"data_dir: str = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:27:40.779700Z","iopub.execute_input":"2023-06-07T22:27:40.780807Z","iopub.status.idle":"2023-06-07T22:27:40.787383Z","shell.execute_reply.started":"2023-06-07T22:27:40.780768Z","shell.execute_reply":"2023-06-07T22:27:40.786476Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:27:40.790572Z","iopub.execute_input":"2023-06-07T22:27:40.791663Z","iopub.status.idle":"2023-06-07T22:27:40.827689Z","shell.execute_reply.started":"2023-06-07T22:27:40.791626Z","shell.execute_reply":"2023-06-07T22:27:40.826722Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"def get_band_images(idx: str, parrent_folder: str, band: str) -> np.array:\n    return np.load(os.path.join(data_dir, parrent_folder, idx, f'band_{band}.npy'))","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:27:40.829173Z","iopub.execute_input":"2023-06-07T22:27:40.829816Z","iopub.status.idle":"2023-06-07T22:27:40.837650Z","shell.execute_reply.started":"2023-06-07T22:27:40.829778Z","shell.execute_reply":"2023-06-07T22:27:40.836625Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"_T11_BOUNDS = (243, 303)\n_CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n_TDIFF_BOUNDS = (-4, 2)\n\ndef normalize_range(data, bounds):\n    \"\"\"Maps data to the range [0, 1].\"\"\"\n    return (data - bounds[0]) / (bounds[1] - bounds[0])\n\n\ndef get_ash_color_images(idx: str, parrent_folder: str, get_mask_frame_only=False) -> np.array:\n    band11 = get_band_images(idx, parrent_folder, '11')\n    band14 = get_band_images(idx, parrent_folder, '14')\n    band15 = get_band_images(idx, parrent_folder, '15')\n    \n    if get_mask_frame_only:\n        band11 = band11[:,:,4]\n        band14 = band14[:,:,4]\n        band15 = band15[:,:,4]\n\n    r = normalize_range(band15 - band14, _TDIFF_BOUNDS)\n    g = normalize_range(band14 - band11, _CLOUD_TOP_TDIFF_BOUNDS)\n    b = normalize_range(band14, _T11_BOUNDS)\n    false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n    return false_color","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:27:40.841210Z","iopub.execute_input":"2023-06-07T22:27:40.841597Z","iopub.status.idle":"2023-06-07T22:27:40.852103Z","shell.execute_reply.started":"2023-06-07T22:27:40.841557Z","shell.execute_reply":"2023-06-07T22:27:40.850744Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_mask_image(idx: str, parrent_folder: str) -> np.array:\n    return np.load(os.path.join(data_dir, parrent_folder, idx, 'human_pixel_masks.npy')) ","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:27:40.855658Z","iopub.execute_input":"2023-06-07T22:27:40.855934Z","iopub.status.idle":"2023-06-07T22:27:40.861910Z","shell.execute_reply.started":"2023-06-07T22:27:40.855910Z","shell.execute_reply":"2023-06-07T22:27:40.860843Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class Upsampled(nn.Module):\n    def __init__(self, back_bone: nn.Module):\n        super(Upsampled, self).__init__()\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n        self.back_bone = back_bone\n        self.down = nn.Upsample(scale_factor=0.5, mode='bilinear', align_corners=False)\n\n    def forward(self, x):\n        # Forward pass through the layers\n        x = self.up(x)\n        x = self.back_bone(x)\n        x = self.down(x)\n        return x\n    \n    def forward_no_downsample_back(self, x):\n        # Forward pass through the layers\n        x = self.up(x)\n        x = self.back_bone(x)\n        return x ","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:27:40.863733Z","iopub.execute_input":"2023-06-07T22:27:40.864187Z","iopub.status.idle":"2023-06-07T22:27:40.874627Z","shell.execute_reply.started":"2023-06-07T22:27:40.864155Z","shell.execute_reply":"2023-06-07T22:27:40.873671Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"core = smp.Unet(\n    encoder_name ='efficientnet-b0',\n    encoder_weights=None,\n    in_channels=3,   \n    classes=1,\n    activation=None#\"sigmoid\",\n    )\nmodel = Upsampled(core)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:27:40.876306Z","iopub.execute_input":"2023-06-07T22:27:40.876792Z","iopub.status.idle":"2023-06-07T22:27:44.184226Z","shell.execute_reply.started":"2023-06-07T22:27:40.876757Z","shell.execute_reply":"2023-06-07T22:27:44.183276Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Upsampled(\n  (up): Upsample(scale_factor=2.0, mode='bilinear')\n  (back_bone): Unet(\n    (encoder): EfficientNetEncoder(\n      (_conv_stem): Conv2dStaticSamePadding(\n        3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n        (static_padding): ZeroPad2d((0, 1, 0, 1))\n      )\n      (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_blocks): ModuleList(\n        (0): MBConvBlock(\n          (_depthwise_conv): Conv2dStaticSamePadding(\n            32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n            (static_padding): ZeroPad2d((1, 1, 1, 1))\n          )\n          (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_se_reduce): Conv2dStaticSamePadding(\n            32, 8, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_se_expand): Conv2dStaticSamePadding(\n            8, 32, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_project_conv): Conv2dStaticSamePadding(\n            32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_swish): MemoryEfficientSwish()\n        )\n        (1): MBConvBlock(\n          (_expand_conv): Conv2dStaticSamePadding(\n            16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_depthwise_conv): Conv2dStaticSamePadding(\n            96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n            (static_padding): ZeroPad2d((0, 1, 0, 1))\n          )\n          (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_se_reduce): Conv2dStaticSamePadding(\n            96, 4, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_se_expand): Conv2dStaticSamePadding(\n            4, 96, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_project_conv): Conv2dStaticSamePadding(\n            96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_swish): MemoryEfficientSwish()\n        )\n        (2): MBConvBlock(\n          (_expand_conv): Conv2dStaticSamePadding(\n            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_depthwise_conv): Conv2dStaticSamePadding(\n            144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n            (static_padding): ZeroPad2d((1, 1, 1, 1))\n          )\n          (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_se_reduce): Conv2dStaticSamePadding(\n            144, 6, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_se_expand): Conv2dStaticSamePadding(\n            6, 144, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_project_conv): Conv2dStaticSamePadding(\n            144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_swish): MemoryEfficientSwish()\n        )\n        (3): MBConvBlock(\n          (_expand_conv): Conv2dStaticSamePadding(\n            24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_depthwise_conv): Conv2dStaticSamePadding(\n            144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n            (static_padding): ZeroPad2d((1, 2, 1, 2))\n          )\n          (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_se_reduce): Conv2dStaticSamePadding(\n            144, 6, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_se_expand): Conv2dStaticSamePadding(\n            6, 144, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_project_conv): Conv2dStaticSamePadding(\n            144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_swish): MemoryEfficientSwish()\n        )\n        (4): MBConvBlock(\n          (_expand_conv): Conv2dStaticSamePadding(\n            40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_depthwise_conv): Conv2dStaticSamePadding(\n            240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n            (static_padding): ZeroPad2d((2, 2, 2, 2))\n          )\n          (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_se_reduce): Conv2dStaticSamePadding(\n            240, 10, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_se_expand): Conv2dStaticSamePadding(\n            10, 240, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_project_conv): Conv2dStaticSamePadding(\n            240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_swish): MemoryEfficientSwish()\n        )\n        (5): MBConvBlock(\n          (_expand_conv): Conv2dStaticSamePadding(\n            40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_depthwise_conv): Conv2dStaticSamePadding(\n            240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n            (static_padding): ZeroPad2d((0, 1, 0, 1))\n          )\n          (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_se_reduce): Conv2dStaticSamePadding(\n            240, 10, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_se_expand): Conv2dStaticSamePadding(\n            10, 240, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_project_conv): Conv2dStaticSamePadding(\n            240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_swish): MemoryEfficientSwish()\n        )\n        (6-7): 2 x MBConvBlock(\n          (_expand_conv): Conv2dStaticSamePadding(\n            80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_depthwise_conv): Conv2dStaticSamePadding(\n            480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n            (static_padding): ZeroPad2d((1, 1, 1, 1))\n          )\n          (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_se_reduce): Conv2dStaticSamePadding(\n            480, 20, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_se_expand): Conv2dStaticSamePadding(\n            20, 480, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_project_conv): Conv2dStaticSamePadding(\n            480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_swish): MemoryEfficientSwish()\n        )\n        (8): MBConvBlock(\n          (_expand_conv): Conv2dStaticSamePadding(\n            80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_depthwise_conv): Conv2dStaticSamePadding(\n            480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n            (static_padding): ZeroPad2d((2, 2, 2, 2))\n          )\n          (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_se_reduce): Conv2dStaticSamePadding(\n            480, 20, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_se_expand): Conv2dStaticSamePadding(\n            20, 480, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_project_conv): Conv2dStaticSamePadding(\n            480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_swish): MemoryEfficientSwish()\n        )\n        (9-10): 2 x MBConvBlock(\n          (_expand_conv): Conv2dStaticSamePadding(\n            112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_depthwise_conv): Conv2dStaticSamePadding(\n            672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n            (static_padding): ZeroPad2d((2, 2, 2, 2))\n          )\n          (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_se_reduce): Conv2dStaticSamePadding(\n            672, 28, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_se_expand): Conv2dStaticSamePadding(\n            28, 672, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_project_conv): Conv2dStaticSamePadding(\n            672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_swish): MemoryEfficientSwish()\n        )\n        (11): MBConvBlock(\n          (_expand_conv): Conv2dStaticSamePadding(\n            112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_depthwise_conv): Conv2dStaticSamePadding(\n            672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n            (static_padding): ZeroPad2d((1, 2, 1, 2))\n          )\n          (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_se_reduce): Conv2dStaticSamePadding(\n            672, 28, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_se_expand): Conv2dStaticSamePadding(\n            28, 672, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_project_conv): Conv2dStaticSamePadding(\n            672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_swish): MemoryEfficientSwish()\n        )\n        (12-14): 3 x MBConvBlock(\n          (_expand_conv): Conv2dStaticSamePadding(\n            192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_depthwise_conv): Conv2dStaticSamePadding(\n            1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n            (static_padding): ZeroPad2d((2, 2, 2, 2))\n          )\n          (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_se_reduce): Conv2dStaticSamePadding(\n            1152, 48, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_se_expand): Conv2dStaticSamePadding(\n            48, 1152, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_project_conv): Conv2dStaticSamePadding(\n            1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_swish): MemoryEfficientSwish()\n        )\n        (15): MBConvBlock(\n          (_expand_conv): Conv2dStaticSamePadding(\n            192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_depthwise_conv): Conv2dStaticSamePadding(\n            1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n            (static_padding): ZeroPad2d((1, 1, 1, 1))\n          )\n          (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_se_reduce): Conv2dStaticSamePadding(\n            1152, 48, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_se_expand): Conv2dStaticSamePadding(\n            48, 1152, kernel_size=(1, 1), stride=(1, 1)\n            (static_padding): Identity()\n          )\n          (_project_conv): Conv2dStaticSamePadding(\n            1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n            (static_padding): Identity()\n          )\n          (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n          (_swish): MemoryEfficientSwish()\n        )\n      )\n      (_conv_head): Conv2dStaticSamePadding(\n        320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n      (_dropout): Dropout(p=0.2, inplace=False)\n      (_swish): MemoryEfficientSwish()\n    )\n    (decoder): UnetDecoder(\n      (center): Identity()\n      (blocks): ModuleList(\n        (0): DecoderBlock(\n          (conv1): Conv2dReLU(\n            (0): Conv2d(432, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention1): Attention(\n            (attention): Identity()\n          )\n          (conv2): Conv2dReLU(\n            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention2): Attention(\n            (attention): Identity()\n          )\n        )\n        (1): DecoderBlock(\n          (conv1): Conv2dReLU(\n            (0): Conv2d(296, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention1): Attention(\n            (attention): Identity()\n          )\n          (conv2): Conv2dReLU(\n            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention2): Attention(\n            (attention): Identity()\n          )\n        )\n        (2): DecoderBlock(\n          (conv1): Conv2dReLU(\n            (0): Conv2d(152, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention1): Attention(\n            (attention): Identity()\n          )\n          (conv2): Conv2dReLU(\n            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention2): Attention(\n            (attention): Identity()\n          )\n        )\n        (3): DecoderBlock(\n          (conv1): Conv2dReLU(\n            (0): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention1): Attention(\n            (attention): Identity()\n          )\n          (conv2): Conv2dReLU(\n            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention2): Attention(\n            (attention): Identity()\n          )\n        )\n        (4): DecoderBlock(\n          (conv1): Conv2dReLU(\n            (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention1): Attention(\n            (attention): Identity()\n          )\n          (conv2): Conv2dReLU(\n            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (attention2): Attention(\n            (attention): Identity()\n          )\n        )\n      )\n    )\n    (segmentation_head): SegmentationHead(\n      (0): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): Identity()\n      (2): Activation(\n        (activation): Identity()\n      )\n    )\n  )\n  (down): Upsample(scale_factor=0.5, mode='bilinear')\n)"},"metadata":{}}]},{"cell_type":"code","source":"#model.load_state_dict(torch.load('/kaggle/working/model_checkpoint_e1.pt'))\n#model.eval()\n#model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:27:44.188713Z","iopub.execute_input":"2023-06-07T22:27:44.189643Z","iopub.status.idle":"2023-06-07T22:27:44.194278Z","shell.execute_reply.started":"2023-06-07T22:27:44.189613Z","shell.execute_reply":"2023-06-07T22:27:44.193088Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"summary(model, input_size=(3, 256, 256))","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:27:44.195824Z","iopub.execute_input":"2023-06-07T22:27:44.196265Z","iopub.status.idle":"2023-06-07T22:27:49.569775Z","shell.execute_reply.started":"2023-06-07T22:27:44.196218Z","shell.execute_reply":"2023-06-07T22:27:49.568711Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n          Upsample-1          [-1, 3, 512, 512]               0\n         ZeroPad2d-2          [-1, 3, 513, 513]               0\nConv2dStaticSamePadding-3         [-1, 32, 256, 256]             864\n       BatchNorm2d-4         [-1, 32, 256, 256]              64\nMemoryEfficientSwish-5         [-1, 32, 256, 256]               0\n         ZeroPad2d-6         [-1, 32, 258, 258]               0\nConv2dStaticSamePadding-7         [-1, 32, 256, 256]             288\n       BatchNorm2d-8         [-1, 32, 256, 256]              64\nMemoryEfficientSwish-9         [-1, 32, 256, 256]               0\n         Identity-10             [-1, 32, 1, 1]               0\nConv2dStaticSamePadding-11              [-1, 8, 1, 1]             264\nMemoryEfficientSwish-12              [-1, 8, 1, 1]               0\n         Identity-13              [-1, 8, 1, 1]               0\nConv2dStaticSamePadding-14             [-1, 32, 1, 1]             288\n         Identity-15         [-1, 32, 256, 256]               0\nConv2dStaticSamePadding-16         [-1, 16, 256, 256]             512\n      BatchNorm2d-17         [-1, 16, 256, 256]              32\n      MBConvBlock-18         [-1, 16, 256, 256]               0\n         Identity-19         [-1, 16, 256, 256]               0\nConv2dStaticSamePadding-20         [-1, 96, 256, 256]           1,536\n      BatchNorm2d-21         [-1, 96, 256, 256]             192\nMemoryEfficientSwish-22         [-1, 96, 256, 256]               0\n        ZeroPad2d-23         [-1, 96, 257, 257]               0\nConv2dStaticSamePadding-24         [-1, 96, 128, 128]             864\n      BatchNorm2d-25         [-1, 96, 128, 128]             192\nMemoryEfficientSwish-26         [-1, 96, 128, 128]               0\n         Identity-27             [-1, 96, 1, 1]               0\nConv2dStaticSamePadding-28              [-1, 4, 1, 1]             388\nMemoryEfficientSwish-29              [-1, 4, 1, 1]               0\n         Identity-30              [-1, 4, 1, 1]               0\nConv2dStaticSamePadding-31             [-1, 96, 1, 1]             480\n         Identity-32         [-1, 96, 128, 128]               0\nConv2dStaticSamePadding-33         [-1, 24, 128, 128]           2,304\n      BatchNorm2d-34         [-1, 24, 128, 128]              48\n      MBConvBlock-35         [-1, 24, 128, 128]               0\n         Identity-36         [-1, 24, 128, 128]               0\nConv2dStaticSamePadding-37        [-1, 144, 128, 128]           3,456\n      BatchNorm2d-38        [-1, 144, 128, 128]             288\nMemoryEfficientSwish-39        [-1, 144, 128, 128]               0\n        ZeroPad2d-40        [-1, 144, 130, 130]               0\nConv2dStaticSamePadding-41        [-1, 144, 128, 128]           1,296\n      BatchNorm2d-42        [-1, 144, 128, 128]             288\nMemoryEfficientSwish-43        [-1, 144, 128, 128]               0\n         Identity-44            [-1, 144, 1, 1]               0\nConv2dStaticSamePadding-45              [-1, 6, 1, 1]             870\nMemoryEfficientSwish-46              [-1, 6, 1, 1]               0\n         Identity-47              [-1, 6, 1, 1]               0\nConv2dStaticSamePadding-48            [-1, 144, 1, 1]           1,008\n         Identity-49        [-1, 144, 128, 128]               0\nConv2dStaticSamePadding-50         [-1, 24, 128, 128]           3,456\n      BatchNorm2d-51         [-1, 24, 128, 128]              48\n      MBConvBlock-52         [-1, 24, 128, 128]               0\n         Identity-53         [-1, 24, 128, 128]               0\nConv2dStaticSamePadding-54        [-1, 144, 128, 128]           3,456\n      BatchNorm2d-55        [-1, 144, 128, 128]             288\nMemoryEfficientSwish-56        [-1, 144, 128, 128]               0\n        ZeroPad2d-57        [-1, 144, 131, 131]               0\nConv2dStaticSamePadding-58          [-1, 144, 64, 64]           3,600\n      BatchNorm2d-59          [-1, 144, 64, 64]             288\nMemoryEfficientSwish-60          [-1, 144, 64, 64]               0\n         Identity-61            [-1, 144, 1, 1]               0\nConv2dStaticSamePadding-62              [-1, 6, 1, 1]             870\nMemoryEfficientSwish-63              [-1, 6, 1, 1]               0\n         Identity-64              [-1, 6, 1, 1]               0\nConv2dStaticSamePadding-65            [-1, 144, 1, 1]           1,008\n         Identity-66          [-1, 144, 64, 64]               0\nConv2dStaticSamePadding-67           [-1, 40, 64, 64]           5,760\n      BatchNorm2d-68           [-1, 40, 64, 64]              80\n      MBConvBlock-69           [-1, 40, 64, 64]               0\n         Identity-70           [-1, 40, 64, 64]               0\nConv2dStaticSamePadding-71          [-1, 240, 64, 64]           9,600\n      BatchNorm2d-72          [-1, 240, 64, 64]             480\nMemoryEfficientSwish-73          [-1, 240, 64, 64]               0\n        ZeroPad2d-74          [-1, 240, 68, 68]               0\nConv2dStaticSamePadding-75          [-1, 240, 64, 64]           6,000\n      BatchNorm2d-76          [-1, 240, 64, 64]             480\nMemoryEfficientSwish-77          [-1, 240, 64, 64]               0\n         Identity-78            [-1, 240, 1, 1]               0\nConv2dStaticSamePadding-79             [-1, 10, 1, 1]           2,410\nMemoryEfficientSwish-80             [-1, 10, 1, 1]               0\n         Identity-81             [-1, 10, 1, 1]               0\nConv2dStaticSamePadding-82            [-1, 240, 1, 1]           2,640\n         Identity-83          [-1, 240, 64, 64]               0\nConv2dStaticSamePadding-84           [-1, 40, 64, 64]           9,600\n      BatchNorm2d-85           [-1, 40, 64, 64]              80\n      MBConvBlock-86           [-1, 40, 64, 64]               0\n         Identity-87           [-1, 40, 64, 64]               0\nConv2dStaticSamePadding-88          [-1, 240, 64, 64]           9,600\n      BatchNorm2d-89          [-1, 240, 64, 64]             480\nMemoryEfficientSwish-90          [-1, 240, 64, 64]               0\n        ZeroPad2d-91          [-1, 240, 65, 65]               0\nConv2dStaticSamePadding-92          [-1, 240, 32, 32]           2,160\n      BatchNorm2d-93          [-1, 240, 32, 32]             480\nMemoryEfficientSwish-94          [-1, 240, 32, 32]               0\n         Identity-95            [-1, 240, 1, 1]               0\nConv2dStaticSamePadding-96             [-1, 10, 1, 1]           2,410\nMemoryEfficientSwish-97             [-1, 10, 1, 1]               0\n         Identity-98             [-1, 10, 1, 1]               0\nConv2dStaticSamePadding-99            [-1, 240, 1, 1]           2,640\n        Identity-100          [-1, 240, 32, 32]               0\nConv2dStaticSamePadding-101           [-1, 80, 32, 32]          19,200\n     BatchNorm2d-102           [-1, 80, 32, 32]             160\n     MBConvBlock-103           [-1, 80, 32, 32]               0\n        Identity-104           [-1, 80, 32, 32]               0\nConv2dStaticSamePadding-105          [-1, 480, 32, 32]          38,400\n     BatchNorm2d-106          [-1, 480, 32, 32]             960\nMemoryEfficientSwish-107          [-1, 480, 32, 32]               0\n       ZeroPad2d-108          [-1, 480, 34, 34]               0\nConv2dStaticSamePadding-109          [-1, 480, 32, 32]           4,320\n     BatchNorm2d-110          [-1, 480, 32, 32]             960\nMemoryEfficientSwish-111          [-1, 480, 32, 32]               0\n        Identity-112            [-1, 480, 1, 1]               0\nConv2dStaticSamePadding-113             [-1, 20, 1, 1]           9,620\nMemoryEfficientSwish-114             [-1, 20, 1, 1]               0\n        Identity-115             [-1, 20, 1, 1]               0\nConv2dStaticSamePadding-116            [-1, 480, 1, 1]          10,080\n        Identity-117          [-1, 480, 32, 32]               0\nConv2dStaticSamePadding-118           [-1, 80, 32, 32]          38,400\n     BatchNorm2d-119           [-1, 80, 32, 32]             160\n     MBConvBlock-120           [-1, 80, 32, 32]               0\n        Identity-121           [-1, 80, 32, 32]               0\nConv2dStaticSamePadding-122          [-1, 480, 32, 32]          38,400\n     BatchNorm2d-123          [-1, 480, 32, 32]             960\nMemoryEfficientSwish-124          [-1, 480, 32, 32]               0\n       ZeroPad2d-125          [-1, 480, 34, 34]               0\nConv2dStaticSamePadding-126          [-1, 480, 32, 32]           4,320\n     BatchNorm2d-127          [-1, 480, 32, 32]             960\nMemoryEfficientSwish-128          [-1, 480, 32, 32]               0\n        Identity-129            [-1, 480, 1, 1]               0\nConv2dStaticSamePadding-130             [-1, 20, 1, 1]           9,620\nMemoryEfficientSwish-131             [-1, 20, 1, 1]               0\n        Identity-132             [-1, 20, 1, 1]               0\nConv2dStaticSamePadding-133            [-1, 480, 1, 1]          10,080\n        Identity-134          [-1, 480, 32, 32]               0\nConv2dStaticSamePadding-135           [-1, 80, 32, 32]          38,400\n     BatchNorm2d-136           [-1, 80, 32, 32]             160\n     MBConvBlock-137           [-1, 80, 32, 32]               0\n        Identity-138           [-1, 80, 32, 32]               0\nConv2dStaticSamePadding-139          [-1, 480, 32, 32]          38,400\n     BatchNorm2d-140          [-1, 480, 32, 32]             960\nMemoryEfficientSwish-141          [-1, 480, 32, 32]               0\n       ZeroPad2d-142          [-1, 480, 36, 36]               0\nConv2dStaticSamePadding-143          [-1, 480, 32, 32]          12,000\n     BatchNorm2d-144          [-1, 480, 32, 32]             960\nMemoryEfficientSwish-145          [-1, 480, 32, 32]               0\n        Identity-146            [-1, 480, 1, 1]               0\nConv2dStaticSamePadding-147             [-1, 20, 1, 1]           9,620\nMemoryEfficientSwish-148             [-1, 20, 1, 1]               0\n        Identity-149             [-1, 20, 1, 1]               0\nConv2dStaticSamePadding-150            [-1, 480, 1, 1]          10,080\n        Identity-151          [-1, 480, 32, 32]               0\nConv2dStaticSamePadding-152          [-1, 112, 32, 32]          53,760\n     BatchNorm2d-153          [-1, 112, 32, 32]             224\n     MBConvBlock-154          [-1, 112, 32, 32]               0\n        Identity-155          [-1, 112, 32, 32]               0\nConv2dStaticSamePadding-156          [-1, 672, 32, 32]          75,264\n     BatchNorm2d-157          [-1, 672, 32, 32]           1,344\nMemoryEfficientSwish-158          [-1, 672, 32, 32]               0\n       ZeroPad2d-159          [-1, 672, 36, 36]               0\nConv2dStaticSamePadding-160          [-1, 672, 32, 32]          16,800\n     BatchNorm2d-161          [-1, 672, 32, 32]           1,344\nMemoryEfficientSwish-162          [-1, 672, 32, 32]               0\n        Identity-163            [-1, 672, 1, 1]               0\nConv2dStaticSamePadding-164             [-1, 28, 1, 1]          18,844\nMemoryEfficientSwish-165             [-1, 28, 1, 1]               0\n        Identity-166             [-1, 28, 1, 1]               0\nConv2dStaticSamePadding-167            [-1, 672, 1, 1]          19,488\n        Identity-168          [-1, 672, 32, 32]               0\nConv2dStaticSamePadding-169          [-1, 112, 32, 32]          75,264\n     BatchNorm2d-170          [-1, 112, 32, 32]             224\n     MBConvBlock-171          [-1, 112, 32, 32]               0\n        Identity-172          [-1, 112, 32, 32]               0\nConv2dStaticSamePadding-173          [-1, 672, 32, 32]          75,264\n     BatchNorm2d-174          [-1, 672, 32, 32]           1,344\nMemoryEfficientSwish-175          [-1, 672, 32, 32]               0\n       ZeroPad2d-176          [-1, 672, 36, 36]               0\nConv2dStaticSamePadding-177          [-1, 672, 32, 32]          16,800\n     BatchNorm2d-178          [-1, 672, 32, 32]           1,344\nMemoryEfficientSwish-179          [-1, 672, 32, 32]               0\n        Identity-180            [-1, 672, 1, 1]               0\nConv2dStaticSamePadding-181             [-1, 28, 1, 1]          18,844\nMemoryEfficientSwish-182             [-1, 28, 1, 1]               0\n        Identity-183             [-1, 28, 1, 1]               0\nConv2dStaticSamePadding-184            [-1, 672, 1, 1]          19,488\n        Identity-185          [-1, 672, 32, 32]               0\nConv2dStaticSamePadding-186          [-1, 112, 32, 32]          75,264\n     BatchNorm2d-187          [-1, 112, 32, 32]             224\n     MBConvBlock-188          [-1, 112, 32, 32]               0\n        Identity-189          [-1, 112, 32, 32]               0\nConv2dStaticSamePadding-190          [-1, 672, 32, 32]          75,264\n     BatchNorm2d-191          [-1, 672, 32, 32]           1,344\nMemoryEfficientSwish-192          [-1, 672, 32, 32]               0\n       ZeroPad2d-193          [-1, 672, 35, 35]               0\nConv2dStaticSamePadding-194          [-1, 672, 16, 16]          16,800\n     BatchNorm2d-195          [-1, 672, 16, 16]           1,344\nMemoryEfficientSwish-196          [-1, 672, 16, 16]               0\n        Identity-197            [-1, 672, 1, 1]               0\nConv2dStaticSamePadding-198             [-1, 28, 1, 1]          18,844\nMemoryEfficientSwish-199             [-1, 28, 1, 1]               0\n        Identity-200             [-1, 28, 1, 1]               0\nConv2dStaticSamePadding-201            [-1, 672, 1, 1]          19,488\n        Identity-202          [-1, 672, 16, 16]               0\nConv2dStaticSamePadding-203          [-1, 192, 16, 16]         129,024\n     BatchNorm2d-204          [-1, 192, 16, 16]             384\n     MBConvBlock-205          [-1, 192, 16, 16]               0\n        Identity-206          [-1, 192, 16, 16]               0\nConv2dStaticSamePadding-207         [-1, 1152, 16, 16]         221,184\n     BatchNorm2d-208         [-1, 1152, 16, 16]           2,304\nMemoryEfficientSwish-209         [-1, 1152, 16, 16]               0\n       ZeroPad2d-210         [-1, 1152, 20, 20]               0\nConv2dStaticSamePadding-211         [-1, 1152, 16, 16]          28,800\n     BatchNorm2d-212         [-1, 1152, 16, 16]           2,304\nMemoryEfficientSwish-213         [-1, 1152, 16, 16]               0\n        Identity-214           [-1, 1152, 1, 1]               0\nConv2dStaticSamePadding-215             [-1, 48, 1, 1]          55,344\nMemoryEfficientSwish-216             [-1, 48, 1, 1]               0\n        Identity-217             [-1, 48, 1, 1]               0\nConv2dStaticSamePadding-218           [-1, 1152, 1, 1]          56,448\n        Identity-219         [-1, 1152, 16, 16]               0\nConv2dStaticSamePadding-220          [-1, 192, 16, 16]         221,184\n     BatchNorm2d-221          [-1, 192, 16, 16]             384\n     MBConvBlock-222          [-1, 192, 16, 16]               0\n        Identity-223          [-1, 192, 16, 16]               0\nConv2dStaticSamePadding-224         [-1, 1152, 16, 16]         221,184\n     BatchNorm2d-225         [-1, 1152, 16, 16]           2,304\nMemoryEfficientSwish-226         [-1, 1152, 16, 16]               0\n       ZeroPad2d-227         [-1, 1152, 20, 20]               0\nConv2dStaticSamePadding-228         [-1, 1152, 16, 16]          28,800\n     BatchNorm2d-229         [-1, 1152, 16, 16]           2,304\nMemoryEfficientSwish-230         [-1, 1152, 16, 16]               0\n        Identity-231           [-1, 1152, 1, 1]               0\nConv2dStaticSamePadding-232             [-1, 48, 1, 1]          55,344\nMemoryEfficientSwish-233             [-1, 48, 1, 1]               0\n        Identity-234             [-1, 48, 1, 1]               0\nConv2dStaticSamePadding-235           [-1, 1152, 1, 1]          56,448\n        Identity-236         [-1, 1152, 16, 16]               0\nConv2dStaticSamePadding-237          [-1, 192, 16, 16]         221,184\n     BatchNorm2d-238          [-1, 192, 16, 16]             384\n     MBConvBlock-239          [-1, 192, 16, 16]               0\n        Identity-240          [-1, 192, 16, 16]               0\nConv2dStaticSamePadding-241         [-1, 1152, 16, 16]         221,184\n     BatchNorm2d-242         [-1, 1152, 16, 16]           2,304\nMemoryEfficientSwish-243         [-1, 1152, 16, 16]               0\n       ZeroPad2d-244         [-1, 1152, 20, 20]               0\nConv2dStaticSamePadding-245         [-1, 1152, 16, 16]          28,800\n     BatchNorm2d-246         [-1, 1152, 16, 16]           2,304\nMemoryEfficientSwish-247         [-1, 1152, 16, 16]               0\n        Identity-248           [-1, 1152, 1, 1]               0\nConv2dStaticSamePadding-249             [-1, 48, 1, 1]          55,344\nMemoryEfficientSwish-250             [-1, 48, 1, 1]               0\n        Identity-251             [-1, 48, 1, 1]               0\nConv2dStaticSamePadding-252           [-1, 1152, 1, 1]          56,448\n        Identity-253         [-1, 1152, 16, 16]               0\nConv2dStaticSamePadding-254          [-1, 192, 16, 16]         221,184\n     BatchNorm2d-255          [-1, 192, 16, 16]             384\n     MBConvBlock-256          [-1, 192, 16, 16]               0\n        Identity-257          [-1, 192, 16, 16]               0\nConv2dStaticSamePadding-258         [-1, 1152, 16, 16]         221,184\n     BatchNorm2d-259         [-1, 1152, 16, 16]           2,304\nMemoryEfficientSwish-260         [-1, 1152, 16, 16]               0\n       ZeroPad2d-261         [-1, 1152, 18, 18]               0\nConv2dStaticSamePadding-262         [-1, 1152, 16, 16]          10,368\n     BatchNorm2d-263         [-1, 1152, 16, 16]           2,304\nMemoryEfficientSwish-264         [-1, 1152, 16, 16]               0\n        Identity-265           [-1, 1152, 1, 1]               0\nConv2dStaticSamePadding-266             [-1, 48, 1, 1]          55,344\nMemoryEfficientSwish-267             [-1, 48, 1, 1]               0\n        Identity-268             [-1, 48, 1, 1]               0\nConv2dStaticSamePadding-269           [-1, 1152, 1, 1]          56,448\n        Identity-270         [-1, 1152, 16, 16]               0\nConv2dStaticSamePadding-271          [-1, 320, 16, 16]         368,640\n     BatchNorm2d-272          [-1, 320, 16, 16]             640\n     MBConvBlock-273          [-1, 320, 16, 16]               0\nEfficientNetEncoder-274  [[-1, 3, 512, 512], [-1, 32, 256, 256], [-1, 24, 128, 128], [-1, 40, 64, 64], [-1, 112, 32, 32], [-1, 320, 16, 16]]               0\n        Identity-275          [-1, 320, 16, 16]               0\n        Identity-276          [-1, 432, 32, 32]               0\n       Attention-277          [-1, 432, 32, 32]               0\n          Conv2d-278          [-1, 256, 32, 32]         995,328\n     BatchNorm2d-279          [-1, 256, 32, 32]             512\n            ReLU-280          [-1, 256, 32, 32]               0\n          Conv2d-281          [-1, 256, 32, 32]         589,824\n     BatchNorm2d-282          [-1, 256, 32, 32]             512\n            ReLU-283          [-1, 256, 32, 32]               0\n        Identity-284          [-1, 256, 32, 32]               0\n       Attention-285          [-1, 256, 32, 32]               0\n    DecoderBlock-286          [-1, 256, 32, 32]               0\n        Identity-287          [-1, 296, 64, 64]               0\n       Attention-288          [-1, 296, 64, 64]               0\n          Conv2d-289          [-1, 128, 64, 64]         340,992\n     BatchNorm2d-290          [-1, 128, 64, 64]             256\n            ReLU-291          [-1, 128, 64, 64]               0\n          Conv2d-292          [-1, 128, 64, 64]         147,456\n     BatchNorm2d-293          [-1, 128, 64, 64]             256\n            ReLU-294          [-1, 128, 64, 64]               0\n        Identity-295          [-1, 128, 64, 64]               0\n       Attention-296          [-1, 128, 64, 64]               0\n    DecoderBlock-297          [-1, 128, 64, 64]               0\n        Identity-298        [-1, 152, 128, 128]               0\n       Attention-299        [-1, 152, 128, 128]               0\n          Conv2d-300         [-1, 64, 128, 128]          87,552\n     BatchNorm2d-301         [-1, 64, 128, 128]             128\n            ReLU-302         [-1, 64, 128, 128]               0\n          Conv2d-303         [-1, 64, 128, 128]          36,864\n     BatchNorm2d-304         [-1, 64, 128, 128]             128\n            ReLU-305         [-1, 64, 128, 128]               0\n        Identity-306         [-1, 64, 128, 128]               0\n       Attention-307         [-1, 64, 128, 128]               0\n    DecoderBlock-308         [-1, 64, 128, 128]               0\n        Identity-309         [-1, 96, 256, 256]               0\n       Attention-310         [-1, 96, 256, 256]               0\n          Conv2d-311         [-1, 32, 256, 256]          27,648\n     BatchNorm2d-312         [-1, 32, 256, 256]              64\n            ReLU-313         [-1, 32, 256, 256]               0\n          Conv2d-314         [-1, 32, 256, 256]           9,216\n     BatchNorm2d-315         [-1, 32, 256, 256]              64\n            ReLU-316         [-1, 32, 256, 256]               0\n        Identity-317         [-1, 32, 256, 256]               0\n       Attention-318         [-1, 32, 256, 256]               0\n    DecoderBlock-319         [-1, 32, 256, 256]               0\n          Conv2d-320         [-1, 16, 512, 512]           4,608\n     BatchNorm2d-321         [-1, 16, 512, 512]              32\n            ReLU-322         [-1, 16, 512, 512]               0\n          Conv2d-323         [-1, 16, 512, 512]           2,304\n     BatchNorm2d-324         [-1, 16, 512, 512]              32\n            ReLU-325         [-1, 16, 512, 512]               0\n        Identity-326         [-1, 16, 512, 512]               0\n       Attention-327         [-1, 16, 512, 512]               0\n    DecoderBlock-328         [-1, 16, 512, 512]               0\n     UnetDecoder-329         [-1, 16, 512, 512]               0\n          Conv2d-330          [-1, 1, 512, 512]             145\n        Identity-331          [-1, 1, 512, 512]               0\n        Identity-332          [-1, 1, 512, 512]               0\n      Activation-333          [-1, 1, 512, 512]               0\n            Unet-334          [-1, 1, 512, 512]               0\n        Upsample-335          [-1, 1, 256, 256]               0\n================================================================\nTotal params: 5,839,309\nTrainable params: 5,839,309\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.75\nForward/backward pass size (MB): 1849.60\nParams size (MB): 22.28\nEstimated Total Size (MB): 1872.62\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Trainer","metadata":{"execution":{"iopub.status.busy":"2023-05-31T21:02:32.133730Z","iopub.execute_input":"2023-05-31T21:02:32.134168Z","iopub.status.idle":"2023-05-31T21:02:32.138968Z","shell.execute_reply.started":"2023-05-31T21:02:32.134136Z","shell.execute_reply":"2023-05-31T21:02:32.137968Z"}}},{"cell_type":"code","source":"class Dice(nn.Module):\n    def __init__(self, use_sigmoid=True):\n        super(Dice, self).__init__()\n        self.sigmoid = nn.Sigmoid()\n        self.use_sigmoid = use_sigmoid\n\n    def forward(self, inputs, targets, smooth=1):\n        if self.use_sigmoid:\n            inputs = self.sigmoid(inputs)       \n        \n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()\n        dice = (2.0 *intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        \n        return dice\n    \ndice = Dice()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:27:49.571562Z","iopub.execute_input":"2023-06-07T22:27:49.572200Z","iopub.status.idle":"2023-06-07T22:27:49.581253Z","shell.execute_reply.started":"2023-06-07T22:27:49.572164Z","shell.execute_reply":"2023-06-07T22:27:49.579877Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class MyTrainer:\n    def __init__(self, model, optimizer, loss_fn, lr_scheduler):\n        self.validation_losses = []\n        self.batch_losses = []\n        self.epoch_losses = []\n        self.learning_rates = []\n        self.model = model\n        self.optimizer = optimizer\n        self.loss_fn = loss_fn\n        self.lr_scheduler = lr_scheduler\n        self._check_optim_net_aligned()\n\n    # Ensures that the given optimizer points to the given model\n    def _check_optim_net_aligned(self):\n        assert self.optimizer.param_groups[0]['params'] == list(self.model.parameters())\n\n    # Trains the model\n    def fit(self,\n            train_dataloader: DataLoader,\n            test_dataloader: DataLoader,\n            epochs: int = 10,\n            eval_every: int = 1,\n            ):\n  \n        for e in range(epochs):\n            print(\"New learning rate: {}\".format(self.lr_scheduler.get_last_lr()))\n            self.learning_rates.append(self.lr_scheduler.get_last_lr()[0])\n\n            # Stores data about the batch\n            batch_losses = []\n            sub_batch_losses = []\n\n            for i, data in enumerate(train_dataloader):\n                self.model.train()\n                if i % 100 == 0:\n                    print(f'epotch: {e} batch: {i}/{len(train_dataloader)} loss: {torch.Tensor(sub_batch_losses).mean()}')\n                    sub_batch_losses.clear()\n                # Every data instance is an input + label pair\n                images, mask = data\n                \n                images = images.to(device)\n                mask = mask.to(device)\n\n                # Zero your gradients for every batch!\n                self.optimizer.zero_grad()\n                # Make predictions for this batch\n                outputs = self.model(images)#['out']\n                # Compute the loss and its gradients\n                loss = self.loss_fn(outputs, mask)\n                loss.backward()\n                # Adjust learning weights\n                self.optimizer.step()\n\n                # Saves data\n                self.batch_losses.append(loss.item())\n                batch_losses.append(loss)\n                sub_batch_losses.append(loss)\n\n            # Adjusts learning rate\n            if self.lr_scheduler is not None:\n                self.lr_scheduler.step()\n\n            # Reports on the path\n            mean_epoch_loss = torch.Tensor(batch_losses).mean()\n            self.epoch_losses.append(mean_epoch_loss.item())\n            print('Train Epoch: {} Average Loss: {:.6f}'.format(e, mean_epoch_loss))\n\n            # Reports on the training progress\n            if (e + 1) % eval_every == 0:\n                torch.save(self.model.state_dict(), \"model_checkpoint_e\" + str(e) + \".pt\")\n                with torch.no_grad():\n                    self.model.eval()\n                    losses = []\n                    for i, data in enumerate(test_dataloader):\n                        # Every data instance is an input + label pair\n                        images, mask = data\n\n                        images = images.to(device)\n                        mask = mask.to(device)\n\n                        output = self.model(images)#['out']\n                        loss = self.loss_fn(output, mask)\n                        losses.append(loss.item())\n                        \n                    avg_loss = torch.Tensor(losses).mean().item()\n                    self.validation_losses.append(avg_loss)\n                    print(\"Validation loss after\", (e + 1), \"epochs was\", round(avg_loss, 4))","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:27:49.583102Z","iopub.execute_input":"2023-06-07T22:27:49.583604Z","iopub.status.idle":"2023-06-07T22:27:49.604772Z","shell.execute_reply.started":"2023-06-07T22:27:49.583565Z","shell.execute_reply":"2023-06-07T22:27:49.603534Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class ContrailsAshDataset(torch.utils.data.Dataset):\n    def __init__(self, parrent_folder: str):\n        self.df_idx: pd.DataFrame = pd.DataFrame({'idx': os.listdir(f'/kaggle/input/google-research-identify-contrails-reduce-global-warming/{parrent_folder}')})\n        self.parrent_folder: str = parrent_folder\n\n    def __len__(self):\n        return len(self.df_idx)\n\n    def __getitem__(self, idx):\n        image_id: str = str(self.df_idx.iloc[idx]['idx'])\n        images = torch.tensor(np.reshape(get_ash_color_images(image_id, self.parrent_folder, get_mask_frame_only=True), (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n        mask = torch.tensor(get_mask_image(image_id, self.parrent_folder)).to(torch.float32).permute(2, 0, 1)\n        return images, mask","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:27:49.606419Z","iopub.execute_input":"2023-06-07T22:27:49.607666Z","iopub.status.idle":"2023-06-07T22:27:49.619588Z","shell.execute_reply.started":"2023-06-07T22:27:49.607626Z","shell.execute_reply":"2023-06-07T22:27:49.618406Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"dataset_train = ContrailsAshDataset('train')\ndataset_validation = ContrailsAshDataset('validation')\n\ndata_loader_train = DataLoader(dataset_train, batch_size=16, shuffle=True, num_workers=2, drop_last=True)\ndata_loader_validation = DataLoader(dataset_validation, batch_size=16, shuffle=True, num_workers=2, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:27:49.621090Z","iopub.execute_input":"2023-06-07T22:27:49.622023Z","iopub.status.idle":"2023-06-07T22:27:49.934585Z","shell.execute_reply.started":"2023-06-07T22:27:49.621985Z","shell.execute_reply":"2023-06-07T22:27:49.933474Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"len(os.listdir(f'/kaggle/input/google-research-identify-contrails-reduce-global-warming/train'))","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:27:49.936237Z","iopub.execute_input":"2023-06-07T22:27:49.937125Z","iopub.status.idle":"2023-06-07T22:27:49.955911Z","shell.execute_reply.started":"2023-06-07T22:27:49.937085Z","shell.execute_reply":"2023-06-07T22:27:49.954769Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"20529"},"metadata":{}}]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(10))# smp.losses.DiceLoss(mode='binary') \noptimizer = optim.Adam(model.parameters(), lr=5e-4)\nlr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)\nmodel.train()\n\nnum_epochs = 7\n\ntrainer = MyTrainer(model, optimizer, criterion, lr_scheduler)\ntrainer.fit(data_loader_train, data_loader_validation, epochs=num_epochs)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:27:49.957380Z","iopub.execute_input":"2023-06-07T22:27:49.957762Z","iopub.status.idle":"2023-06-07T22:29:47.630349Z","shell.execute_reply.started":"2023-06-07T22:27:49.957736Z","shell.execute_reply":"2023-06-07T22:29:47.626794Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"New learning rate: [0.0005]\nepotch: 0 batch: 0/1283 loss: nan\nepotch: 0 batch: 10/1283 loss: 0.5986009240150452\nepotch: 0 batch: 20/1283 loss: 0.40431323647499084\nepotch: 0 batch: 30/1283 loss: 0.30684125423431396\nepotch: 0 batch: 40/1283 loss: 0.2558574676513672\nepotch: 0 batch: 50/1283 loss: 0.23649191856384277\nepotch: 0 batch: 60/1283 loss: 0.23285862803459167\nepotch: 0 batch: 70/1283 loss: 0.21275082230567932\nepotch: 0 batch: 80/1283 loss: 0.20246577262878418\nepotch: 0 batch: 90/1283 loss: 0.1882757693529129\nepotch: 0 batch: 100/1283 loss: 0.18319836258888245\nepotch: 0 batch: 110/1283 loss: 0.2171717882156372\nepotch: 0 batch: 120/1283 loss: 0.16159948706626892\nepotch: 0 batch: 130/1283 loss: 0.15907007455825806\nepotch: 0 batch: 140/1283 loss: 0.14879387617111206\nepotch: 0 batch: 150/1283 loss: 0.16033130884170532\nepotch: 0 batch: 160/1283 loss: 0.15710127353668213\nepotch: 0 batch: 170/1283 loss: 0.17864295840263367\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m\n\u001b[1;32m      8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MyTrainer(model, optimizer, criterion, lr_scheduler)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[13], line 47\u001b[0m, in \u001b[0;36mMyTrainer.fit\u001b[0;34m(self, train_dataloader, test_dataloader, epochs, eval_every)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Make predictions for this batch\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#['out']\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Compute the loss and its gradients\u001b[39;00m\n\u001b[1;32m     49\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(outputs, mask)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[8], line 11\u001b[0m, in \u001b[0;36mUpsampled.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Forward pass through the layers\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup(x)\n\u001b[0;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mback_bone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown(x)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segmentation_models_pytorch/base/model.py:29\u001b[0m, in \u001b[0;36mSegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input_shape(x)\n\u001b[0;32m---> 29\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[1;32m     32\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_head(decoder_output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segmentation_models_pytorch/encoders/efficientnet.py:73\u001b[0m, in \u001b[0;36mEfficientNetEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m             drop_connect \u001b[38;5;241m=\u001b[39m drop_connect_rate \u001b[38;5;241m*\u001b[39m block_number \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocks)\n\u001b[1;32m     72\u001b[0m             block_number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m---> 73\u001b[0m             x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_connect\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/efficientnet_pytorch/model.py:109\u001b[0m, in \u001b[0;36mMBConvBlock.forward\u001b[0;34m(self, inputs, drop_connect_rate)\u001b[0m\n\u001b[1;32m    106\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bn0(x)\n\u001b[1;32m    107\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swish(x)\n\u001b[0;32m--> 109\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_depthwise_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bn1(x)\n\u001b[1;32m    111\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swish(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/efficientnet_pytorch/utils.py:274\u001b[0m, in \u001b[0;36mConv2dStaticSamePadding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 274\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatic_padding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mconv2d(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/padding.py:25\u001b[0m, in \u001b[0;36m_ConstantPadNd.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconstant\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"# Find Optimal Threshold","metadata":{}},{"cell_type":"code","source":"class DiceThresholdTester:\n    \n    def __init__(self, model: nn.Module, data_loader: torch.utils.data.DataLoader):\n        self.model = model\n        self.data_loader = data_loader\n        self.cumulative_mask_pred = []\n        self.cumulative_mask_true = []\n        \n    def precalculate_prediction(self) -> None:\n        self.model.eval()\n        with torch.no_grad():\n            sigmoid = nn.Sigmoid()\n\n            for images, mask_true in self.data_loader:\n                if torch.cuda.is_available():\n                    images = images.cuda()\n\n                mask_pred = sigmoid(model.forward(images))\n\n                self.cumulative_mask_pred.append(mask_pred.cpu().detach().numpy())\n                self.cumulative_mask_true.append(mask_true.cpu().detach().numpy())\n\n            self.cumulative_mask_pred = np.concatenate(self.cumulative_mask_pred, axis=0)\n            self.cumulative_mask_true = np.concatenate(self.cumulative_mask_true, axis=0)\n\n            self.cumulative_mask_pred = torch.flatten(torch.from_numpy(self.cumulative_mask_pred))\n            self.cumulative_mask_true = torch.flatten(torch.from_numpy(self.cumulative_mask_true))\n    \n    def test_threshold(self, threshold: float) -> float:\n        _dice = Dice(use_sigmoid=False)\n        after_threshold = np.zeros(self.cumulative_mask_pred.shape)\n        after_threshold[self.cumulative_mask_pred[:] > threshold] = 1\n        after_threshold[self.cumulative_mask_pred[:] < threshold] = 0\n        after_threshold = torch.flatten(torch.from_numpy(after_threshold))\n        return _dice(self.cumulative_mask_true, after_threshold).item()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:29:47.634270Z","iopub.status.idle":"2023-06-07T22:29:47.637034Z","shell.execute_reply.started":"2023-06-07T22:29:47.636747Z","shell.execute_reply":"2023-06-07T22:29:47.636775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dice_threshold_tester = DiceThresholdTester(model, data_loader_validation)\ndice_threshold_tester.precalculate_prediction()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:29:47.641696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresholds_to_test = [round(x * 0.01, 2) for x in range(101)]\n\noptim_threshold = 0.975\nbest_dice_score = -1\n\nthresholds = []\ndice_scores = []\n\nfor t in thresholds_to_test:\n    dice_score = dice_threshold_tester.test_threshold(t)\n    if dice_score > best_dice_score:\n        best_dice_score = dice_score\n        optim_threshold = t\n    \n    thresholds.append(t)\n    dice_scores.append(dice_score)\n    \nprint(f'Best Threshold: {optim_threshold} with dice: {best_dice_score}')\ndf_threshold_data = pd.DataFrame({'Threshold': thresholds, 'Dice Score': dice_scores})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(data=df_threshold_data, x='Threshold', y='Dice Score')\nplt.axhline(y=best_dice_score, color='green')\nplt.axvline(x=optim_threshold, color='green')\nplt.text(-0.02, best_dice_score * 0.96, f'{best_dice_score:.3f}', va='center', ha='left', color='green')\nplt.text(optim_threshold - 0.01, 0.02, f'{optim_threshold}', va='center', ha='right', color='green')\nplt.ylim(bottom=0)\nplt.title('Threshold vs Dice Score')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate","metadata":{}},{"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nbatches_to_show = 4\nmodel.eval()\n\nfor i, data in enumerate(data_loader_validation):\n    images, mask = data\n    \n    # Predict mask for this instance\n    if torch.cuda.is_available():\n        images = images.cuda()\n    predicated_mask = sigmoid(model.forward(images[:, :, :, :]).cpu().detach().numpy())\n    \n    # Apply threshold\n    predicated_mask_with_threshold = np.zeros((images.shape[0], 256, 256))\n    predicated_mask_with_threshold[predicated_mask[:, 0, :, :] < optim_threshold] = 0\n    predicated_mask_with_threshold[predicated_mask[:, 0, :, :] > optim_threshold] = 1\n    \n    images = images.cpu()\n        \n    for img_num in range(0, images.shape[0]):\n        fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20,10))\n        axes = axes.flatten()\n        \n        # Show groud trought \n        axes[0].imshow(mask[img_num, 0, :, :])\n        axes[0].axis('off')\n        axes[0].set_title('Ground Truth')\n        \n        # Show ash color scheme input image\n        axes[1].imshow( np.concatenate(\n            (\n            np.expand_dims(images[img_num, 4, :, :], axis=2),\n            np.expand_dims(images[img_num, 12, :, :], axis=2),\n            np.expand_dims(images[img_num, 20, :, :], axis=2)\n        ), axis=2))\n        axes[1].axis('off')\n        axes[1].set_title('Ash color scheeme input - Frame 4')\n\n        # Show predicted mask\n        axes[2].imshow(predicated_mask[img_num, 0, :, :], vmin=0, vmax=1)\n        axes[2].axis('off')\n        axes[2].set_title('Predicted probability mask')\n\n        # Show predicted mask after threshold\n        axes[3].imshow(predicated_mask_with_threshold[img_num, :, :])\n        axes[3].axis('off')\n        axes[3].set_title('Predicted mask with threshold')\n        plt.show()\n    \n    if i + 1 >= batches_to_show:\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}